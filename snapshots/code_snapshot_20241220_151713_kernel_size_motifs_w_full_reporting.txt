
================================================================================
FILE: snapshots/txt_snap.py
================================================================================
import os
from datetime import datetime

# Root directory containing the code
ROOT_DIR = "/storage1/fs1/dspencer/Active/spencerlab/abonney/EpiBench"

# File extensions to include in the snapshot
FILE_EXTENSIONS = ['.py', '.sh', '.R', '.ipynb', '.yml', '.yaml']

def should_include(file_name):
    # Exclude macOS resource fork files that start with '._'
    if file_name.startswith("._"):
        return False
    # Include file if it has one of the allowed extensions
    return any(file_name.lower().endswith(ext) for ext in FILE_EXTENSIONS)

def create_text_snapshot(root_dir, extensions):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_name = f"code_snapshot_{timestamp}.txt"
    snapshot_path = os.path.join(os.getcwd(), snapshot_name)

    with open(snapshot_path, 'w', encoding='utf-8', errors='replace') as out_file:
        # Recursively walk the root directory
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if should_include(file):
                    full_path = os.path.join(root, file)
                    # Write a header for each file before its content
                    relative_path = os.path.relpath(full_path, start=root_dir)
                    out_file.write("\n" + "="*80 + "\n")
                    out_file.write(f"FILE: {relative_path}\n")
                    out_file.write("="*80 + "\n")

                    # Append file contents
                    try:
                        with open(full_path, 'r', encoding='utf-8', errors='replace') as f:
                            content = f.read()
                        out_file.write(content)
                        out_file.write("\n")  # Blank line after file content
                    except Exception as e:
                        out_file.write(f"\n[Error reading file: {e}]\n")

    print(f"Text snapshot created: {snapshot_path}")

if __name__ == "__main__":
    create_text_snapshot(ROOT_DIR, FILE_EXTENSIONS)


================================================================================
FILE: snapshots/snapshot.py
================================================================================
import os
import tarfile
from datetime import datetime

# Root directory containing the code
ROOT_DIR = "/storage1/fs1/dspencer/Active/spencerlab/abonney/EpiBench"

# File extensions to include in the snapshot
FILE_EXTENSIONS = ['.py', '.sh', '.R', '.ipynb', '.yml', '.yaml', '.json', '.txt', '.md']

def should_include(file_name):
    # Include file if it has one of the allowed extensions
    return any(file_name.lower().endswith(ext) for ext in FILE_EXTENSIONS)

def create_snapshot(root_dir, extensions):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_name = f"code_snapshot_{timestamp}.tar.gz"
    snapshot_path = os.path.join(os.getcwd(), snapshot_name)

    # Open the tar.gz file for writing
    with tarfile.open(snapshot_path, "w:gz") as tar:
        # Recursively walk the root directory
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if should_include(file):
                    full_path = os.path.join(root, file)
                    # Add file to the archive, preserving directory structure relative to ROOT_DIR
                    arcname = os.path.relpath(full_path, start=root_dir)
                    tar.add(full_path, arcname=arcname)

    print(f"Snapshot created: {snapshot_path}")

if __name__ == "__main__":
    create_snapshot(ROOT_DIR, FILE_EXTENSIONS)


================================================================================
FILE: src/__init__.py
================================================================================
# src/__init__.py
# This top-level __init__ can import frequently used items to make them accessible directly from `src`.
# For example, import the config variables at the top level.

from .config import (
    RAW_BED_FILE,
    RAW_FASTA_FILE,
    PREPARED_DATA_CSV,
    FEATURE_DATA_CSV,
    RANDOM_SEED
)

# This way, I can do: from src import PREPARED_DATA_CSV


================================================================================
FILE: src/config.py
================================================================================
# config.py

##########################
# File Paths
##########################
RAW_BED_FILE = "data/raw/ND150826-CD34-wgbs.positive_control_regions.meth.bed"
RAW_FASTA_FILE = "data/raw/positive_control_sequences.fa"
PREPARED_DATA_CSV = "data/processed/positive_control_data.csv"
FEATURE_DATA_CSV = "data/processed/positive_control_features.csv"

##########################
# General Settings
##########################
RANDOM_SEED = 42  # For reproducibility


================================================================================
FILE: src/reporting/run_report.py
================================================================================
# run_report.py
import json
import os
from src.reporting.generate_report import generate_html_report


# Load saved metrics and plots
with open("report/regression_metrics.json", 'r') as f:
    regression_results = json.load(f)

with open("report/regression_plots.json", 'r') as f:
    regression_plots = json.load(f)

with open("report/classification_metrics.json", 'r') as f:
    classification_results = json.load(f)

with open("report/classification_plots.json", 'r') as f:
    classification_plots = json.load(f)

with open("report/data_exploration_plots.json", 'r') as f:
    data_exploration_plots = json.load(f)

# Combine regression and classification plots and also keep data exploration separate
# Here we do not prepend 'report/' to keep paths relative from index.html
all_plots = {}
all_plots.update(regression_plots)
all_plots.update(classification_plots)

generate_html_report(
    regression_results=regression_results,
    classification_results=classification_results,
    data_exploration_plots=data_exploration_plots,
    plots=all_plots,
    output_path="report/combined_report.html"
)

print("Combined report generated at: report/combined_report.html")

================================================================================
FILE: src/reporting/__init__.py
================================================================================
# src/reporting/__init__.py
from .generate_report import generate_html_report

================================================================================
FILE: src/reporting/generate_report.py
================================================================================
import os
import re
import glob

def generate_html_report(
    regression_results,
    classification_results,
    data_exploration_plots,
    plots,
    output_path
):
    """
    Generate a comprehensive HTML report with:
    - Data exploration plots (feature distributions in a grid)
    - Regression & classification results
    - Integrated Gradients explainability
    - General motif discovery
    - Motif discovery by kernel size
    """

    # Map plot keys to human-readable titles
    plot_titles = {
        "correlation_heatmap": "Feature and Methylation Score Correlation Heatmap",
        "test_score_distribution": "Distribution of Test Methylation Scores",
        "cnn_pred_vs_actual": "CNN Predictions vs Actual Methylation Scores",
        "class_label_distribution": "Distribution of Class Labels",
        "lr_roc": "Logistic Regression ROC Curve",
        "rf_roc": "Random Forest ROC Curve",
        "mlp_roc": "MLP ROC Curve",
        "ensemble_roc": "Ensemble ROC Curve",
        "ig_heatmap": "Integrated Gradients Attribution Heatmap"
    }

    regression_plot_keys = ["test_score_distribution", "cnn_pred_vs_actual"]
    classification_plot_keys = ["class_label_distribution", "lr_roc", "rf_roc", "mlp_roc", "ensemble_roc"]

    # Construct regression results table
    regression_table = (
        "<table class='metric-table'>"
        "<tr><th>Model</th><th>MSE</th><th>RÂ²</th><th>Pearson Corr</th></tr>"
    )
    for model, metrics in regression_results.items():
        regression_table += (
            f"<tr><td>{model}</td>"
            f"<td>{metrics['mse']:.4f}</td>"
            f"<td>{metrics['r2']:.4f}</td>"
            f"<td>{metrics['pearson_corr']:.4f}</td></tr>"
        )
    regression_table += "</table>"

    # Construct classification results table
    classification_table = (
        "<table class='metric-table'>"
        "<tr><th>Model</th><th>AUC</th><th>Accuracy</th>"
        "<th>F1</th><th>Precision</th><th>Recall</th></tr>"
    )
    for model, metrics in classification_results.items():
        classification_table += (
            f"<tr><td>{model}</td>"
            f"<td>{metrics.get('auc', 0.0):.4f}</td>"
            f"<td>{metrics.get('accuracy',0.0):.4f}</td>"
            f"<td>{metrics.get('f1',0.0):.4f}</td>"
            f"<td>{metrics.get('precision',0.0):.4f}</td>"
            f"<td>{metrics.get('recall',0.0):.4f}</td></tr>"
        )
    classification_table += "</table>"

    # Data exploration section
    data_exploration_html = "<h2 id='data-exploration'>Data and Feature Exploration</h2>"
    data_exploration_html += (
        "<p>This section provides insights into the relationships between features and methylation score, "
        "including a correlation heatmap and feature distributions.</p>"
    )
    if "correlation_heatmap" in data_exploration_plots:
        corr_path = data_exploration_plots["correlation_heatmap"]
        title = plot_titles.get("correlation_heatmap", "Feature Correlation Heatmap")
        data_exploration_html += f"<h3>{title}</h3><img src='{corr_path}' alt='{title}' class='plot-img'>"
    if "feature_distributions" in data_exploration_plots:
        data_exploration_html += "<h3>Feature Distributions</h3>"
        data_exploration_html += "<p>Below are histograms showing the distributions of various numeric features.</p>"
        data_exploration_html += "<div class='feature-dists'>"
        for feature, fpath in data_exploration_plots["feature_distributions"].items():
            data_exploration_html += f"<div class='feature-dist-block'><h4>{feature}</h4><img src='{fpath}' alt='{feature}_distribution' class='plot-img feature-img'></div>"
        data_exploration_html += "</div>"

    # Regression section
    regression_html = (
        "<h2 id='regression-performance'>Regression Performance</h2>"
        "<p>The table below presents performance metrics for regression models predicting continuous methylation levels. "
        "Following the table are plots illustrating regression model predictions and score distributions.</p>"
        f"{regression_table}"
    )

    # Classification section
    classification_html = (
        "<h2 id='classification-performance'>Classification Performance</h2>"
        "<p>The table below presents performance metrics for classification models predicting whether samples exceed a given methylation score threshold. "
        "Following the table are ROC curves and distribution plots.</p>"
        f"{classification_table}"
    )

    # Regression plots
    regression_plots_html = "<h2>Regression Plots</h2>"
    for plot_name in regression_plot_keys:
        if plot_name in plots:
            formatted_name = plot_titles.get(plot_name, plot_name.replace('_', ' ').title())
            regression_plots_html += f"<h3>{formatted_name}</h3><img src='{plots[plot_name]}' alt='{formatted_name}' class='plot-img'>"

    # Check if there's an interactive version of the CNN plot
    if "cnn_pred_vs_actual_interactive" in plots:
        regression_plots_html += (
            "<h3>Interactive Version</h3>"
            f"<p><a href='{plots['cnn_pred_vs_actual_interactive']}' target='_blank' class='interactive-link'>Open Interactive Plot</a></p>"
        )

    # Classification plots
    classification_plots_html = "<h2>Classification Plots</h2>"
    for plot_name in classification_plot_keys:
        if plot_name in plots:
            formatted_name = plot_titles.get(plot_name, plot_name.replace('_', ' ').title())
            classification_plots_html += f"<h3>{formatted_name}</h3><img src='{plots[plot_name]}' alt='{formatted_name}' class='plot-img'>"

    # Explainability section (Integrated Gradients)
    ig_keys = [k for k in plots.keys() if k.startswith("ig_heatmap_sample_") or k == "ig_heatmap"]

    def extract_index(key):
        match = re.search(r"ig_heatmap_sample_(\d+)", key)
        if match:
            return int(match.group(1))
        else:
            return -1  # main ig_heatmap first

    ig_keys_sorted = sorted(ig_keys, key=extract_index)

    explainability_html = ""
    if ig_keys_sorted:
        explainability_html += "<h2 id='explainability'>Explainability (Integrated Gradients)</h2>"
        explainability_html += (
            "<p>The Integrated Gradients approach helps interpret the model's predictions by attributing contributions "
            "of each nucleotide in the sequence. Red areas indicate features that push predictions higher, and blue areas "
            "indicate features that lower predictions.</p>"
        )
        for ig_key in ig_keys_sorted:
            ig_title = plot_titles.get("ig_heatmap", "Integrated Gradients Attribution Heatmap")
            if ig_key == "ig_heatmap":
                explainability_html += f"<h3>{ig_title}</h3>"
            else:
                sample_idx = extract_index(ig_key)
                explainability_html += f"<h3>{ig_title} (Sample {sample_idx})</h3>"
            explainability_html += f"<img src='{plots[ig_key]}' alt='{ig_title}' class='plot-img'>"

    # Original Motif discovery section
    motif_html = ""
    logos_dir = "report/motifs/clusters/logos"
    if os.path.exists(logos_dir) and os.listdir(logos_dir):
        motif_html += "<h2 id='motif-discovery'>Motif Discovery</h2>"
        motif_html += (
            "<p>Motif logos represent enriched sequence patterns discovered from highly attributed subsequences. "
            "These patterns may highlight biologically relevant features influencing model predictions.</p>"
        )

        logo_files = glob.glob(os.path.join(logos_dir, "*.png"))
        if logo_files:
            motif_html += "<div class='motif-logos'>"
            for logo in sorted(logo_files):
                logo_name = os.path.basename(logo)
                cluster_id = re.search(r"cluster_(\d+)_logo", logo_name)
                cluster_id_str = cluster_id.group(1) if cluster_id else "Unknown"
                motif_html += f"<div class='motif-logo-block'><h3>Motif Cluster {cluster_id_str}</h3>"
                motif_html += f"<img src='motifs/clusters/logos/{logo_name}' alt='Motif Logo Cluster {cluster_id_str}' class='plot-img motif-img'></div>"
            motif_html += "</div>"
        else:
            motif_html += "<p>No motif logos found.</p>"

    # Motif discovery by kernel size section
    kernel_motifs_html = ""
    motifs_by_kernel_dir = "report/motifs_by_kernel"
    if os.path.exists(motifs_by_kernel_dir) and os.listdir(motifs_by_kernel_dir):
        kernel_motifs_html += "<h2 id='kernel-motif-discovery'>Motif Discovery by Kernel Size</h2>"
        kernel_motifs_html += (
            "<p>Below are motif logos discovered for each kernel size used in the model. "
            "These motifs may highlight sequence patterns relevant to the convolutional filters.</p>"
        )

        # Find all kernel directories
        kernel_dirs = glob.glob(os.path.join(motifs_by_kernel_dir, "kernel_*"))
        if kernel_dirs:
            # Sort by kernel size (extracting number from directory name)
            kernel_dirs = sorted(kernel_dirs, key=lambda x: int(re.search(r'kernel_(\d+)', x).group(1)))
            for k_dir in kernel_dirs:
                k_name = os.path.basename(k_dir)  # e.g. kernel_5
                kernel_motifs_html += f"<h3>{k_name.capitalize()}</h3>"
                cluster_logos_dir = os.path.join(k_dir, "clusters", "logos")
                if os.path.exists(cluster_logos_dir) and os.listdir(cluster_logos_dir):
                    logo_files = glob.glob(os.path.join(cluster_logos_dir, "*.png"))
                    if logo_files:
                        kernel_motifs_html += "<div class='motif-logos'>"
                        for logo in sorted(logo_files):
                            logo_name = os.path.basename(logo)
                            cluster_id = re.search(r"cluster_(\d+)_logo", logo_name)
                            cluster_id_str = cluster_id.group(1) if cluster_id else "Unknown"
                            kernel_motifs_html += f"<div class='motif-logo-block'><h4>Cluster {cluster_id_str}</h4>"
                            kernel_motifs_html += f"<img src='motifs_by_kernel/{k_name}/clusters/logos/{logo_name}' alt='Motif Logo {k_name} Cluster {cluster_id_str}' class='plot-img motif-img'></div>"
                        kernel_motifs_html += "</div>"
                    else:
                        kernel_motifs_html += "<p>No logos found for this kernel size.</p>"
                else:
                    kernel_motifs_html += "<p>No logos found for this kernel size.</p>"
        else:
            kernel_motifs_html += "<p>No kernel motif directories found.</p>"

    # Combine everything into a single HTML
    html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Methylation Prediction Report</title>
<style>
body {{
    font-family: "Helvetica Neue", Arial, sans-serif;
    margin: 0;
    padding: 0;
    color: #333;
    background-color: #f7f7f7;
}}
header {{
    background: #4a90e2;
    padding: 20px;
    color: #fff;
    text-align: center;
}}
h1, h2, h3, h4 {{
    font-weight: 500;
    color: #333;
}}
.container {{
    display: flex;
}}
.sidebar {{
    width: 220px;
    background: #2c3e50;
    color: #fff;
    padding: 20px;
    position: fixed;
    top: 64px; /* header height */
    bottom: 0;
    overflow-y: auto;
}}
.sidebar h2 {{
    font-size: 1.1em;
    margin-top: 0;
}}
.sidebar a {{
    display: block;
    color: #ecf0f1;
    text-decoration: none;
    margin: 10px 0;
    font-size: 0.95em;
}}
.sidebar a:hover {{
    text-decoration: underline;
}}
.content {{
    margin-left: 240px;
    padding: 40px;
    max-width: 1200px;
}}
.metric-table {{
    border-collapse: collapse;
    margin: 20px 0;
    width: 100%;
    background: #fff;
    border-radius: 4px;
    overflow: hidden;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
}}
.metric-table th, .metric-table td {{
    border: 1px solid #eee;
    padding: 10px 12px;
    text-align: center;
    font-size: 0.95em;
}}
.metric-table th {{
    background-color: #f0f0f0;
    font-weight: 600;
}}
.plot-img {{
    max-width: 600px;
    height: auto;
    display: block;
    margin: 20px 0;
    border: 1px solid #ccc;
    background: #fff;
    padding: 5px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}}
.feature-dists {{
    display: flex;
    flex-wrap: wrap;
    gap: 30px;
    margin: 20px 0;
}}
.feature-dist-block {{
    background: #fff;
    border-radius: 4px;
    padding: 10px;
    text-align: center;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    flex: 0 0 auto;
}}
.feature-img {{
    max-width: 250px; /* slightly smaller for grid layout */
}}
.feature-dist-block h4 {{
    margin-top: 0;
    font-size: 1em;
}}
.interactive-link {{
    color: #2c3e50;
    font-weight: 600;
}}
.motif-logos {{
    display: flex;
    flex-wrap: wrap;
    gap: 30px;
    margin: 20px 0;
}}
.motif-logo-block {{
    background: #fff;
    border-radius: 4px;
    padding: 10px;
    text-align: center;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}}
.motif-img {{
    max-width: 300px;
}}
section {{
    margin-bottom: 60px;
}}
p {{
    line-height: 1.6em;
}}
</style>
</head>
<body>
<header>
    <h1>Methylation Prediction Report</h1>
    <p style="margin:0;font-size:1.1em;">A comprehensive overview of regression and classification models, data exploration, explainability, and motif discovery.</p>
</header>
<div class="container">
    <div class="sidebar">
        <h2>Navigation</h2>
        <a href="#data-exploration">Data Exploration</a>
        <a href="#regression-performance">Regression Performance</a>
        <a href="#classification-performance">Classification Performance</a>
        <a href="#explainability">Explainability (IG)</a>
        <a href="#motif-discovery">Motif Discovery</a>
        <a href="#kernel-motif-discovery">Motif Discovery by Kernel Size</a>
    </div>
    <div class="content">
        <section>
            <p>
            This report presents a comprehensive overview of our analysis pipeline, including data and feature exploration, regression and classification modeling for methylation prediction, integrated gradients-based explainability, and motif discovery (both general and kernel-specific).
            </p>
        </section>

        {data_exploration_html}

        {regression_html}

        {regression_plots_html}

        {classification_html}

        {classification_plots_html}

        {explainability_html}

        {motif_html}

        {kernel_motifs_html}

        <section>
        <h2>Conclusions</h2>
        <p>
        In summary, our Random Forest approach often provides robust performance on classification tasks, while the CNN-based regressor shows promising results in predicting continuous methylation scores.
        Integrated Gradients analysis provides insights into which nucleotide positions the model relies on, guiding potential feature engineering and improvements.
        Motif discovery (both general and kernel-specific) highlights enriched sequence patterns that may be biologically relevant. Further exploration and validation of these motifs could uncover important regulatory mechanisms.
        </p>
        </section>
    </div>
</div>
</body>
</html>
"""

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w') as f:
        f.write(html_content)

    print(f"Report generated at: {output_path}")


================================================================================
FILE: src/utils/__init__.py
================================================================================
# src/utils/__init__.py
# Re-export utilities for easy access.
from .file_utils import load_csv, save_csv
from .seq_utils import gc_content, cpg_density, kmer_counts


================================================================================
FILE: src/utils/seq_utils.py
================================================================================
# seq_utils.py
from itertools import product

def gc_content(seq):
    seq = seq.upper()
    gc_count = seq.count('G') + seq.count('C')
    return gc_count / len(seq) if len(seq) > 0 else 0

def cpg_density(seq):
    seq = seq.upper()
    cpg_count = seq.count("CG")
    return cpg_count / len(seq) if len(seq) > 0 else 0

def kmer_counts(seq, k=2):
    seq = seq.upper()
    counts = {}
    possible_kmers = [''.join(p) for p in product('ACGT', repeat=k)]
    for km in possible_kmers:
        counts[km] = 0
    
    for i in range(len(seq)-k+1):
        kmer = seq[i:i+k]
        if kmer in counts:
            counts[kmer] += 1
    
    length = len(seq)
    if length > 0:
        for km in counts:
            counts[km] = counts[km] / (length - k + 1)
    return counts


================================================================================
FILE: src/utils/file_utils.py
================================================================================
# file_utils.py
import pandas as pd
import os

def load_csv(path):
    """
    Load a CSV file into a pandas DataFrame.
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"File not found: {path}")
    return pd.read_csv(path)

def save_csv(df, path, index=False):
    """
    Save a pandas DataFrame to a CSV file.
    """
    directory = os.path.dirname(path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory)
    df.to_csv(path, index=index)
    print(f"Saved CSV to: {path}")


================================================================================
FILE: src/models/classical.py
================================================================================
# classical.py

import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

def train_linear_regression(X, y):
    model = LinearRegression()
    model.fit(X, y)
    return model

def predict_linear_regression(model, X):
    return model.predict(X)

def train_random_forest_regressor(X, y, n_estimators=100, random_state=42):
    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)
    model.fit(X, y)
    return model

def predict_random_forest_regressor(model, X):
    return model.predict(X)

def train_logistic_regression(X, y):
    model = LogisticRegression(solver='liblinear', random_state=42)
    model.fit(X, y)
    return model

def predict_logistic_regression(model, X):
    # return probabilities for classification tasks
    return model.predict_proba(X)[:, 1]

def train_random_forest_classifier(X, y, n_estimators=100, random_state=42):
    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)
    model.fit(X, y)
    return model

def predict_random_forest_classifier(model, X):
    return model.predict_proba(X)[:, 1]


================================================================================
FILE: src/models/__init__.py
================================================================================
# src/models/__init__.py
# Re-export model training functions and classes to access them easily.

from .classical import (
    train_linear_regression, predict_linear_regression,
    train_random_forest_regressor, predict_random_forest_regressor,
    train_logistic_regression, predict_logistic_regression,
    train_random_forest_classifier, predict_random_forest_classifier
)

from .deep_learning import (
    SeqCNNRegressor, train_model, predict_model,
)

from .datasets import SequenceDataset


================================================================================
FILE: src/models/deep_learning_transformer.py
================================================================================
# deep_learning.py

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import math

PAD_IDX = 4  # Same as in datasets.py

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=10000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :]
        return self.dropout(x)

class DNARegressor(nn.Module):
    def __init__(self, vocab_size=5, embed_dim=64, num_heads=4, num_layers=4, ff_dim=256, dropout=0.1):
        super(DNARegressor, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)
        self.pos_encoder = PositionalEncoding(d_model=embed_dim, dropout=dropout)

        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim,
                                                   nhead=num_heads,
                                                   dim_feedforward=ff_dim,
                                                   dropout=dropout,
                                                   batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.fc1 = nn.Linear(embed_dim, 64)
        self.fc2 = nn.Linear(64, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, attention_mask):
        # x: (batch, seq_len)
        # attention_mask: (batch, seq_len), True for token, False for pad
        # For Transformer, src_key_padding_mask: True means this token is pad.
        src_key_padding_mask = ~attention_mask  # invert mask
        emb = self.embedding(x)  # (batch, seq_len, embed_dim)
        emb = self.pos_encoder(emb)
        out = self.transformer_encoder(emb, src_key_padding_mask=src_key_padding_mask)  # (batch, seq_len, embed_dim)

        # Mean pool over non-padding tokens
        mask = attention_mask.unsqueeze(-1)
        out = out * mask
        sum_out = out.sum(dim=1)
        lengths = mask.sum(dim=1)
        pooled = sum_out / lengths.clamp(min=1)

        # Regression head
        x = self.relu(self.fc1(pooled))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

def train_model(model, train_loader, val_loader, epochs=20, lr=0.001, weight_decay=1e-5, device=None, early_stop_patience=5):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    elif isinstance(device, str):
        device = torch.device(device)

    model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))

    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0.0
        for X_batch, y_batch, mask_batch in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device).unsqueeze(1)
            mask_batch = mask_batch.to(device)

            optimizer.zero_grad()
            with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):
                outputs = model(X_batch, mask_batch)
                loss = criterion(outputs, y_batch)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch, mask_batch in val_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device).unsqueeze(1)
                mask_batch = mask_batch.to(device)
                with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):
                    outputs = model(X_batch, mask_batch)
                    loss = criterion(outputs, y_batch)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)
        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

        # Early stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), "best_model.pth")
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    model.load_state_dict(torch.load("best_model.pth"))

def predict_model(model, test_loader, device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    elif isinstance(device, str):
        device = torch.device(device)

    model.to(device)
    model.eval()
    preds = []
    with torch.no_grad():
        for X_batch, _, mask_batch in test_loader:
            X_batch = X_batch.to(device)
            mask_batch = mask_batch.to(device)
            with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):
                outputs = model(X_batch, mask_batch)
            preds.extend(outputs.squeeze(1).cpu().numpy())
    return np.array(preds)


================================================================================
FILE: src/models/deep_learning.py
================================================================================
# src/models/deep_learning.py

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class SeqCNNRegressor(nn.Module):
    def __init__(self, 
                 kernel_sizes=(3,5,7),
                 filters_per_branch=64, 
                 fc_units=64, 
                 dropout_rate=0.5,
                 use_batchnorm=True):
        super(SeqCNNRegressor, self).__init__()
        
        self.kernel_sizes = kernel_sizes
        self.use_batchnorm = use_batchnorm

        # We'll dynamically create the convolution branches based on kernel_sizes
        self.branches = nn.ModuleList()
        self.branch_norms = nn.ModuleList()
        in_channels = 4  # For one-hot encoded sequence 'A,C,G,T'
        for k in self.kernel_sizes:
            conv = nn.Conv1d(in_channels=in_channels, out_channels=filters_per_branch, kernel_size=k, padding=k//2)
            self.branches.append(conv)
            if self.use_batchnorm:
                self.branch_norms.append(nn.BatchNorm1d(filters_per_branch))
            else:
                self.branch_norms.append(nn.Identity())
        
        # After concatenation: total_channels = filters_per_branch * len(kernel_sizes)
        total_channels = filters_per_branch * len(self.kernel_sizes)
        
        # Additional convolution block
        self.conv2 = nn.Conv1d(in_channels=total_channels, out_channels=total_channels*2, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm1d(total_channels*2) if self.use_batchnorm else nn.Identity()
        self.pool = nn.MaxPool1d(2)
        
        self.conv3 = nn.Conv1d(in_channels=total_channels*2, out_channels=total_channels*4, kernel_size=7, padding=3)
        self.bn3 = nn.BatchNorm1d(total_channels*4) if self.use_batchnorm else nn.Identity()
        
        self.fc1 = nn.Linear(total_channels*4, fc_units)
        self.dropout = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(fc_units, 1)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # x shape: (batch, 4, seq_len)
        branch_outputs = []
        for conv, bn in zip(self.branches, self.branch_norms):
            out = conv(x)
            out = bn(out)
            out = self.relu(out)
            branch_outputs.append(out)

        # Concatenate along channels
        x_cat = torch.cat(branch_outputs, dim=1)
        
        x = self.conv2(x_cat)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.pool(x)

        x = self.conv3(x)
        x = self.bn3(x)
        x = self.relu(x)

        # Global average pooling
        x = x.mean(dim=2)  # (batch, channels)
        
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x


def train_model(model, train_loader, val_loader, epochs=50, lr=0.001, weight_decay=1e-5, device=None, early_stop_patience=5):
    """
    Train the given model using the provided train and validation data loaders.

    Parameters:
        model (nn.Module): The regression model to train.
        train_loader (DataLoader): Loader providing (X, y, mask) for training.
        val_loader (DataLoader): Loader providing (X, y, mask) for validation.
        epochs (int): Maximum number of training epochs.
        lr (float): Learning rate for the optimizer.
        weight_decay (float): Weight decay (L2 regularization) for the optimizer.
        device (torch.device): The device to use for training.
        early_stop_patience (int): Early stopping patience in epochs.
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(epochs):
        # Training phase
        model.train()
        total_train_loss = 0.0
        for X_batch, y_batch, mask_batch in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device).unsqueeze(1)  # Make target shape (batch, 1)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch, mask_batch in val_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device).unsqueeze(1)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

        # Early stopping check
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), "best_model.pth")
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break


def predict_model(model, test_loader, device=None):
    """
    Generate predictions for the data provided by the test_loader.

    Parameters:
        model (nn.Module): The trained regression model.
        test_loader (DataLoader): Loader providing (X, y, mask) for the test set.
        device (torch.device): The device to use for inference.

    Returns:
        np.ndarray: The model predictions as a numpy array of shape (num_samples,).
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    preds = []
    with torch.no_grad():
        for X_batch, y_batch, mask_batch in test_loader:
            X_batch = X_batch.to(device)
            outputs = model(X_batch)  # (batch, 1)
            preds.extend(outputs.squeeze(1).cpu().numpy())
    return np.array(preds)


================================================================================
FILE: src/models/datasets.py
================================================================================
import torch
from torch.utils.data import Dataset
import pandas as pd
import numpy as np

class SequenceDataset(Dataset):
    """
    A PyTorch Dataset for DNA sequences for regression tasks without fixed length,
    but with a maximum allowed length to prevent memory issues.
    """
    def __init__(self, csv_path):
        self.df = pd.read_csv(csv_path)
        self.mapping = {'A':0, 'C':1, 'G':2, 'T':3, 'a':0, 'c':1, 'g':2, 't':3}
        self.max_allowed_length = 10000  # Maximum allowed sequence length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        seq = row['sequence'].upper()

        # Truncate the sequence if it exceeds the max allowed length
        if len(seq) > self.max_allowed_length:
            seq = seq[:self.max_allowed_length]

        seq_len = len(seq)
        one_hot = np.zeros((4, seq_len), dtype=np.float32)
        for i, base in enumerate(seq):
            base_idx = self.mapping.get(base, 0)
            one_hot[base_idx, i] = 1.0

        y = float(row['score'])
        return torch.tensor(one_hot), torch.tensor(y, dtype=torch.float32)


def collate_fn(batch):
    seqs = [item[0] for item in batch]  # list of Tensors (4, seq_len)
    targets = [item[1] for item in batch]  # list of Tensors (score)

    max_len = max(seq.shape[1] for seq in seqs)

    padded_seqs = []
    masks = []
    for seq in seqs:
        seq_len = seq.shape[1]
        # Create a mask of length max_len: 1 for real positions, 0 for padding
        mask = torch.zeros((max_len,), dtype=torch.float32)
        mask[:seq_len] = 1.0

        if seq_len < max_len:
            pad_amount = max_len - seq_len
            pad_tensor = torch.zeros((4, pad_amount), dtype=seq.dtype)
            seq = torch.cat([seq, pad_tensor], dim=1)

        padded_seqs.append(seq)
        masks.append(mask)

    X = torch.stack(padded_seqs, dim=0)   # (batch, 4, max_len)
    y = torch.stack(targets, dim=0)       # (batch,)
    mask = torch.stack(masks, dim=0)      # (batch, max_len)

    return X, y, mask


================================================================================
FILE: src/data_preparation/prepare_data.py
================================================================================
import sys
from Bio import SeqIO

# Update these paths as needed
bed_file = "/storage1/fs1/dspencer/Active/spencerlab/abonney/regions/simple_project/ND150826-CD34-wgbs.positive_control_regions.meth.bed"
fasta_file = "/storage1/fs1/dspencer/Active/spencerlab/abonney/regions/simple_project/positive_control_sequences.fa"
output_csv = "positive_control_data.csv"

# Read BED data
# Based on the data you showed, columns are:
# 0: chrom
# 1: start
# 2: end
# 3: coverage (not needed)
# 4: score (between 0 and 1)
regions = []
with open(bed_file, 'r') as bf:
    for line in bf:
        if line.strip():
            parts = line.strip().split('\t')
            chrom = parts[0]
            start = parts[1]
            end = parts[2]
            # coverage = parts[3]  # If you need it, but not required now
            score = float(parts[4])
            regions.append((chrom, start, end, score))

# Create a dictionary keyed by region string (chr:start-end) to score
score_dict = {}
for (chrom, start, end, score) in regions:
    region_key = f"{chrom}:{start}-{end}"
    score_dict[region_key] = score

# Parse FASTA and match sequences to scores
with open(output_csv, 'w') as out:
    out.write("chrom,start,end,score,sequence\n")
    for record in SeqIO.parse(fasta_file, "fasta"):
        # record.id should be something like: chr1:10000-10100
        region = record.id
        seq = str(record.seq)
        chrom = region.split(':')[0]
        coords = region.split(':')[1]
        start = coords.split('-')[0]
        end = coords.split('-')[1]
        score = score_dict.get(region, "unknown")
        out.write(f"{chrom},{start},{end},{score},{seq}\n")


================================================================================
FILE: src/data_preparation/__init__.py
================================================================================
# src/data_preparation/__init__.py

from .prepare_data import main as prepare_data_main
from .feature_engineering import main as feature_engineering_main

================================================================================
FILE: src/data_preparation/feature_engineering.py
================================================================================
# feature_engineering.py
import pandas as pd
from src.config import PREPARED_DATA_CSV, FEATURE_DATA_CSV
from src.utils.seq_utils import gc_content, cpg_density, kmer_counts
from itertools import product

df = pd.read_csv(PREPARED_DATA_CSV)

all_features = []
for _, row in df.iterrows():
    seq = row['sequence']
    features = {
        'chrom': row['chrom'],
        'start': row['start'],
        'end': row['end'],
        'score': row['score'],
        'seq_length': len(seq),
        'gc_content': gc_content(seq),
        'cpg_density': cpg_density(seq)
    }
    
    # Add 2-mer frequencies
    k2_counts = kmer_counts(seq, k=2)
    for kmer, val in k2_counts.items():
        features[f"k2_{kmer}"] = val
    
    # Add 3-mer frequencies
    k3_counts = kmer_counts(seq, k=3)
    for kmer, val in k3_counts.items():
        features[f"k3_{kmer}"] = val
    
    all_features.append(features)

feature_df = pd.DataFrame(all_features)
feature_df.to_csv(FEATURE_DATA_CSV, index=False)


================================================================================
FILE: src/evaluation/__init__.py
================================================================================
# src/evaluation/__init__.py
from .metrics import evaluate_regression, evaluate_classification
from .visualization import (
    plot_distribution, plot_pred_vs_actual, plot_roc_curve, plot_feature_distributions, plot_correlation_heatmap, plot_pred_vs_actual_interactive
)


================================================================================
FILE: src/evaluation/visualization.py
================================================================================
# visualization.py

import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.metrics import roc_curve, auc
import plotly.express as px

def plot_distribution(y, title, save_path):
    plt.figure(figsize=(6,4))
    sns.histplot(y, kde=True, color='blue', bins=20)
    plt.title(title)
    plt.xlabel("Value")
    plt.ylabel("Count")
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def plot_pred_vs_actual(y_true, y_pred, title, save_path):
    plt.figure(figsize=(6,6))
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([0,1],[0,1], 'r--')
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.title(title)
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def plot_pred_vs_actual_interactive(df, title, save_path):
    """
    Create an interactive scatter plot with Plotly, including Start and End in hover data.
    """
    fig = px.scatter(
        df,
        x="True",
        y="Pred",
        hover_data={
            "True": ':.3f',
            "Pred": ':.3f',
            "Sequence": False,
            "Start": True,
            "End": True,
            "SampleIndex": True,
            "IG_Path": True
        },
        title=title
    )

    # Add a reference line y=x
    fig.add_shape(
        type="line", 
        x0=0, x1=1, 
        y0=0, y1=1,
        line=dict(color="Red", dash="dash")
    )

    fig.update_layout(
        xaxis_title="Actual",
        yaxis_title="Predicted",
        xaxis=dict(range=[0,1]),
        yaxis=dict(range=[0,1])
    )

    _ensure_dir_exists(save_path)
    fig.write_html(save_path)

def plot_roc_curve(y_true, y_pred_proba, title, save_path):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6,6))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC area = {roc_auc:.2f}')
    plt.plot([0,1],[0,1], 'r--')
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(title)
    plt.legend(loc="lower right")
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def _ensure_dir_exists(path):
    dir_name = os.path.dirname(path)
    if dir_name and not os.path.exists(dir_name):
        os.makedirs(dir_name)

def plot_correlation_heatmap(df, title, save_path):
    plt.figure(figsize=(10,8))
    corr = df.corr()
    sns.heatmap(corr, annot=False, cmap='coolwarm', square=True)
    plt.title(title)
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def plot_feature_distributions(df, features, output_dir):
    # Create a directory for the individual feature plots
    os.makedirs(output_dir, exist_ok=True)
    plot_paths = {}
    for feature in features:
        plt.figure(figsize=(6,4))
        sns.histplot(df[feature], kde=True, color='blue', bins=20)
        plt.title(f"Distribution of {feature}")
        plt.xlabel(feature)
        plt.ylabel("Count")
        plt.tight_layout()
        save_path = os.path.join(output_dir, f"{feature}_distribution.png")
        plt.savefig(save_path, dpi=150)
        plt.close()
        plot_paths[feature] = save_path
    return plot_paths


================================================================================
FILE: src/evaluation/metrics.py
================================================================================
# metrics.py

from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score
from scipy.stats import pearsonr
import numpy as np

def evaluate_regression(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    if np.std(y_pred) == 0 or np.std(y_true) == 0:
        corr = 0.0
    else:
        corr, _ = pearsonr(y_true, y_pred)
    return {
        'mse': mse,
        'r2': r2,
        'pearson_corr': corr
    }

def evaluate_classification(y_true, y_pred_proba, threshold=0.5):
    y_pred = (y_pred_proba >= threshold).astype(int)
    auc = roc_auc_score(y_true, y_pred_proba)
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    return {
        'auc': auc,
        'accuracy': acc,
        'f1': f1,
        'precision': prec,
        'recall': rec
    }


================================================================================
FILE: src/pipelines/run_data_exploration.py
================================================================================
# run_data_exploration.py

import os
import json
import logging
import pandas as pd
from src.config import FEATURE_DATA_CSV
from src.utils.file_utils import load_csv
from src.evaluation.visualization import plot_correlation_heatmap, plot_feature_distributions

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting data and feature exploration pipeline...")

# Load the feature data
df = load_csv(FEATURE_DATA_CSV)
logger.info("Feature data loaded successfully.")

exclude_cols = ['chrom']
if 'sequence' in df.columns:
    exclude_cols.append('sequence')

numeric_features = [col for col in df.columns if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]

# Create output directories
os.makedirs("report/data_exploration_plots", exist_ok=True)

# Plot correlation heatmap of numeric features
corr_heatmap_path_full = "report/data_exploration_plots/feature_correlation_heatmap.png"
plot_correlation_heatmap(df[numeric_features], "Feature Correlation Heatmap", corr_heatmap_path_full)

# Plot distributions of each numeric feature
feature_dist_dir_full = "report/data_exploration_plots/features"
feature_dist_plots_full = plot_feature_distributions(df, numeric_features, feature_dist_dir_full)

# Remove the 'report/' prefix for JSON storage
corr_heatmap_path = "data_exploration_plots/feature_correlation_heatmap.png"
feature_dist_plots = {f: p.replace("report/", "") for f, p in feature_dist_plots_full.items()}

data_exploration_plots = {
    "correlation_heatmap": corr_heatmap_path,
    "feature_distributions": feature_dist_plots
}

# Save to JSON
with open("report/data_exploration_plots.json", 'w') as f:
    json.dump(data_exploration_plots, f, indent=4)

logger.info("Data exploration plots saved as JSON.")

# Generate text-based summary report
summary_report_path = "report/data_exploration_summary.txt"

desc = df[numeric_features].describe(include='all')
corr_matrix = df[numeric_features].corr()

corr_pairs = corr_matrix.unstack().dropna()
corr_pairs = corr_pairs[corr_pairs.index.get_level_values(0) != corr_pairs.index.get_level_values(1)]
top_positive_correlations = corr_pairs.sort_values(ascending=False).head(10)
top_negative_correlations = corr_pairs.sort_values(ascending=True).head(10)

with open(summary_report_path, "w") as report_file:
    report_file.write("=== Data Exploration Summary ===\n\n")
    report_file.write("**Numeric Features:**\n")
    report_file.write(", ".join(numeric_features) + "\n\n")

    report_file.write("**Summary Statistics (Numeric Features):**\n")
    report_file.write(desc.to_string() + "\n\n")

    report_file.write("**Top 10 Positive Correlations:**\n")
    for (f1, f2), value in top_positive_correlations.items():
        report_file.write(f"{f1} and {f2}: {value:.3f}\n")
    report_file.write("\n")

    report_file.write("**Top 10 Negative Correlations:**\n")
    for (f1, f2), value in top_negative_correlations.items():
        report_file.write(f"{f1} and {f2}: {value:.3f}\n")
    report_file.write("\n")

    report_file.write("**Observations: **\n")
    report_file.write("1. Review correlations to identify strong relationships.\n")
    report_file.write("2. The summary statistics provide distribution insights.\n")
    report_file.write("3. Feature distribution plots are in 'report/data_exploration_plots/features'.\n")

logger.info(f"Data exploration summary saved at {summary_report_path}.")
logger.info("Data exploration pipeline completed.")


================================================================================
FILE: src/pipelines/run_regression.py
================================================================================
# src/pipelines/run_regression.py

import os
import json
import logging
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, Subset
import optuna
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import string
import ast
from itertools import product
import logomaker

from src.config import PREPARED_DATA_CSV
from src.models.datasets import SequenceDataset, collate_fn
from src.models.deep_learning import SeqCNNRegressor, train_model, predict_model
from src.evaluation.metrics import evaluate_regression
from src.evaluation.visualization import (
    plot_distribution, 
    plot_pred_vs_actual, 
    plot_pred_vs_actual_interactive
)
from src.reporting.generate_report import generate_html_report

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting sequence-based regression pipeline...")

batch_size = 32
epochs = 25
early_stop_patience = 5
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Load dataset and create splits
dataset = SequenceDataset(PREPARED_DATA_CSV)
num_samples = len(dataset)
indices = np.arange(num_samples)
np.random.seed(42)
np.random.shuffle(indices)

train_end = int(0.7 * num_samples)
val_end = int(0.85 * num_samples)

train_indices = indices[:train_end]
val_indices = indices[train_end:val_end]
test_indices = indices[val_end:]

train_dataset = Subset(dataset, train_indices)
val_dataset = Subset(dataset, val_indices)
test_dataset = Subset(dataset, test_indices)

train_loader_for_tuning = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_loader_for_tuning = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

def objective(trial):
    # Hyperparameters for training
    lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True)
    weight_decay = trial.suggest_float("weight_decay", 1e-6, 1e-4, log=True)

    # Hyperparameters for model architecture
    filters_per_branch = trial.suggest_int("filters_per_branch", 32, 128, step=32)
    kernel_sizes_str = trial.suggest_categorical("kernel_sizes", ["3,5,7", "3,7", "5,9"])
    kernel_sizes = tuple(map(int, kernel_sizes_str.split(',')))    
    fc_units = trial.suggest_int("fc_units", 32, 128, step=32)
    dropout_rate = trial.suggest_float("dropout_rate", 0.0, 0.7, step=0.1)
    use_batchnorm = trial.suggest_categorical("use_batchnorm", [True, False])

    model = SeqCNNRegressor(
        kernel_sizes=kernel_sizes,
        filters_per_branch=filters_per_branch,
        fc_units=fc_units,
        dropout_rate=dropout_rate,
        use_batchnorm=use_batchnorm
    )

    train_model(model, train_loader_for_tuning, val_loader_for_tuning,
                epochs=epochs, lr=lr, weight_decay=weight_decay,
                device=device, early_stop_patience=early_stop_patience)
    
    model.load_state_dict(torch.load("best_model.pth", map_location=device, weights_only=True))

    y_val_pred = predict_model(model, val_loader_for_tuning, device=device)
    y_val = [y.item() for _, y in val_dataset]
    y_val = np.array(y_val)
    val_metrics = evaluate_regression(y_val, y_val_pred)

    return val_metrics['mse']

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=100, timeout=None)

logger.info(f"Best trial: {study.best_trial.number}")
logger.info(f"Best trial params: {study.best_params}")
logger.info(f"Best trial validation MSE: {study.best_value}")

best_params = study.best_params

# Convert kernel_sizes back to tuple of ints if needed
kernel_sizes_str = best_params["kernel_sizes"]
if isinstance(kernel_sizes_str, str):
    kernel_sizes = ast.literal_eval(kernel_sizes_str)
else:
    kernel_sizes = kernel_sizes_str

final_model = SeqCNNRegressor(
    kernel_sizes=kernel_sizes,
    filters_per_branch=best_params["filters_per_branch"],
    fc_units=best_params["fc_units"],
    dropout_rate=best_params["dropout_rate"],
    use_batchnorm=best_params["use_batchnorm"]
)

train_model(final_model, train_loader_for_tuning, val_loader_for_tuning,
            epochs=50, lr=best_params["lr"],
            weight_decay=best_params["weight_decay"],
            device=device, early_stop_patience=10)

final_model.load_state_dict(torch.load("best_model.pth", map_location=device, weights_only=True))

y_test_pred = predict_model(final_model, test_loader, device=device)
y_test = [y.item() for _, y in test_dataset]
y_test = np.array(y_test)

test_metrics = evaluate_regression(y_test, y_test_pred)
logger.info(f"Test metrics with best hyperparameters: {test_metrics}")

os.makedirs("report/regression_plots", exist_ok=True)
plot_distribution(y_test, "Distribution of Test Scores", "report/regression_plots/test_score_distribution.png")
plot_pred_vs_actual(y_test, y_test_pred, "CNN Predictions", "report/regression_plots/cnn_pred_vs_actual.png")

regression_results = {
    "CNN_Model": test_metrics
}
plots = {
    "test_score_distribution": "regression_plots/test_score_distribution.png",
    "cnn_pred_vs_actual": "regression_plots/cnn_pred_vs_actual.png"
}

classification_results = {}
data_exploration_plots = {}

# === Explainability Integration Start ===
logger.info("Starting Integrated Gradients explainability...")

from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt
import seaborn as sns

final_model.eval()
final_model.to(device)

X_batch, y_batch, mask_batch = next(iter(test_loader))
X_batch = X_batch.to(device)  # shape: (batch, 4, seq_len)
y_batch = y_batch.to(device)

def forward_fn(x):
    return final_model(x)

ig = IntegratedGradients(forward_fn)

os.makedirs("report/explainability", exist_ok=True)

original_df = pd.read_csv(PREPARED_DATA_CSV)
test_df = original_df.iloc[test_indices]

global_sample_idx = 0
for batch_idx, (X_batch, y_batch, mask_batch) in enumerate(test_loader):
    X_batch = X_batch.to(device)
    y_batch = y_batch.to(device)
    
    batch_size_current = X_batch.size(0)
    for i in range(batch_size_current):
        input_sample = X_batch[i:i+1]
        true_score = y_batch[i].item()

        baseline = torch.zeros_like(input_sample)
        attributions = ig.attribute(input_sample, baseline, target=0)
        attributions = attributions.detach().cpu().numpy().squeeze(0)  # (4, seq_len)

        # Retrieve start/end for this sample from test_df
        start_val = test_df.iloc[global_sample_idx]['start']
        end_val = test_df.iloc[global_sample_idx]['end']

        npy_path = f"report/explainability/attributions_sample_{global_sample_idx}.npy"
        np.save(npy_path, attributions)

        attr_plot_path = f"report/explainability/integrated_gradients_heatmap_sample_{global_sample_idx}.png"
        plt.figure(figsize=(12, 3))
        sns.heatmap(attributions, cmap='cividis', center=0, yticklabels=['A','C','G','T'])
        plt.xlabel("Sequence Position")
        plt.ylabel("Nucleotide Channel")
        plt.title(
            f"IG for Sample {global_sample_idx}, True={true_score:.3f}, "
            f"Pred={y_test_pred[global_sample_idx]:.3f}, Start={start_val}, End={end_val}"
        )
        plt.tight_layout()
        plt.savefig(attr_plot_path, dpi=150)
        plt.close()

        plots[f"ig_heatmap_sample_{global_sample_idx}"] = f"explainability/integrated_gradients_heatmap_sample_{global_sample_idx}.png"

        global_sample_idx += 1

# === Explainability Integration End ===

# Construct a DataFrame for the interactive plot
interactive_df = pd.DataFrame({
    "True": y_test,
    "Pred": y_test_pred,
    "Sequence": test_df['sequence'].values,
    "Start": test_df['start'].values,
    "End": test_df['end'].values,
    "SampleIndex": range(len(y_test))
})

# For IG paths, use the same logic as before
ig_paths = []
for i in range(len(y_test)):
    ig_path = f"explainability/integrated_gradients_heatmap_sample_{i}.png"
    if os.path.exists(os.path.join("report", ig_path)):
        ig_paths.append(ig_path)
    else:
        ig_paths.append("No IG available")

interactive_df["IG_Path"] = ig_paths

interactive_plot_path = "report/regression_plots/cnn_pred_vs_actual_interactive.html"
plot_pred_vs_actual_interactive(interactive_df, "CNN Predictions (Interactive)", interactive_plot_path)

plots["cnn_pred_vs_actual_interactive"] = "regression_plots/cnn_pred_vs_actual_interactive.html"

generate_html_report(
    regression_results=regression_results,
    classification_results=classification_results,
    data_exploration_plots=data_exploration_plots,
    plots=plots,
    output_path="report/index.html"
)

logger.info("Sequence-based regression pipeline completed with explainability and interactive plot. Report generated.")

# === Post-Attribution Analysis Start ===
logger.info("Starting motif discovery by kernel size...")

# We will now create clusters for each kernel size used in the final model.
kernel_sizes = final_model.kernel_sizes  # Assuming the model stores this attribute as given.

os.makedirs("report/motifs_by_kernel", exist_ok=True)

top_fraction = 0.0001  # top X% of positions considered "highly attributed"

# NOTE: We need original_sequences and n_samples here.
n_samples = len(y_test)
original_sequences = test_df['sequence'].values

def kmer_profile(sequence, k=3):
    sequence = sequence.upper()
    possible_kmers = [''.join(x) for x in product('ACGT', repeat=k)]
    counts = {kmer:0 for kmer in possible_kmers}
    for i in range(len(sequence)-k+1):
        kmer_seq = sequence[i:i+k]
        if kmer_seq in counts:
            counts[kmer_seq] += 1
    # Normalize
    length = max(1, len(sequence)-k+1)
    for km in counts:
        counts[km] /= length
    return [counts[km] for km in sorted(counts.keys())]

def extract_and_cluster_by_kernel_size(k):
    logger.info(f"Processing kernel size: {k}")

    # For this kernel size, we'll use the kernel size itself as window_size
    window_size = k

    subsequences_k = []
    positions_info_k = []

    # Re-extract subsequences using the chosen window_size
    for i in range(len(y_test)):
        attr_file = f"report/explainability/attributions_sample_{i}.npy"
        if not os.path.exists(attr_file):
            continue
        attributions = np.load(attr_file)  # shape (4, seq_len)
        seq_len = attributions.shape[1]
        
        # Compute per-position importance score as sum of absolute values
        pos_scores = np.sum(np.abs(attributions), axis=0)
        
        # Find top positions based on top_fraction
        threshold = np.quantile(pos_scores, 1 - top_fraction)
        top_positions = np.where(pos_scores >= threshold)[0]

        seq = original_sequences[i]
        for pos in top_positions:
            start_w = max(0, pos - window_size // 2)
            end_w = min(seq_len, pos + window_size // 2 + 1)
            subseq = seq[start_w:end_w]
            subsequences_k.append(subseq)
            positions_info_k.append((i, start_w, end_w))

    # If no subsequences found for this kernel size, just return
    if not subsequences_k:
        logger.info(f"No subsequences found for kernel size {k}. Skipping.")
        return

    # Save subsequences to a FASTA file
    kernel_dir = f"report/motifs_by_kernel/kernel_{k}"
    os.makedirs(kernel_dir, exist_ok=True)
    fasta_path_k = os.path.join(kernel_dir, "extracted_subsequences.fasta")

    with open(fasta_path_k, 'w') as f:
        for idx, subseq in enumerate(subsequences_k):
            f.write(f">Subsequence_{idx}\n{subseq}\n")

    logger.info(f"Extracted {len(subsequences_k)} subsequences for kernel size {k} and saved to {fasta_path_k}.")

    # K-mer feature extraction for clustering
    kmer_k = 3
    feature_vectors_k = [kmer_profile(s, kmer_k) for s in subsequences_k]
    feature_vectors_k = np.array(feature_vectors_k)

    num_subseq_k = len(feature_vectors_k)
    if num_subseq_k > 1:
        # Determine the optimal number of clusters using the silhouette score
        possible_clusters = range(2, min(10, num_subseq_k) + 1)
        best_n_clusters = 2
        best_score = -1
        for n_clust in possible_clusters:
            km = KMeans(n_clusters=n_clust, random_state=42)
            labels_temp = km.fit_predict(feature_vectors_k)
            score = silhouette_score(feature_vectors_k, labels_temp)
            if score > best_score:
                best_score = score
                best_n_clusters = n_clust

        logger.info(f"[Kernel {k}] Selected {best_n_clusters} clusters (Silhouette score: {best_score:.4f})")

        # Perform final clustering with the chosen number of clusters
        clustering = KMeans(n_clusters=best_n_clusters, random_state=42)
        labels = clustering.fit_predict(feature_vectors_k)

        # Write clustered subsequences to separate FASTA files
        cluster_dir = os.path.join(kernel_dir, "clusters")
        os.makedirs(cluster_dir, exist_ok=True)

        for cluster_id in range(best_n_clusters):
            cluster_seqs = [subseq for subseq, lbl in zip(subsequences_k, labels) if lbl == cluster_id]
            cluster_fasta = os.path.join(cluster_dir, f"cluster_{cluster_id}.fasta")
            with open(cluster_fasta, 'w') as cf:
                for i, cs in enumerate(cluster_seqs):
                    cf.write(f">cluster_{cluster_id}_seq_{i}\n{cs}\n")

        logger.info(f"[Kernel {k}] Clustered subsequences and saved to individual FASTA files in {cluster_dir}.")

        # Create a logo for each cluster
        logo_dir = os.path.join(cluster_dir, "logos")
        os.makedirs(logo_dir, exist_ok=True)

        for cluster_id in range(best_n_clusters):
            cluster_seqs = [subseq for subseq, lbl in zip(subsequences_k, labels) if lbl == cluster_id]
            if not cluster_seqs:
                continue
            # Make letter probability matrix
            length_c = min([len(s) for s in cluster_seqs])
            cluster_seqs = [s[:length_c] for s in cluster_seqs]
            counts = {base:[0]*length_c for base in ['A','C','G','T']}
            for s in cluster_seqs:
                for i, base in enumerate(s.upper()):
                    if base in counts:
                        counts[base][i] += 1

            total_seqs = len(cluster_seqs)
            for base in counts:
                counts[base] = [x/total_seqs for x in counts[base]]

            df_logo = pd.DataFrame(counts)

            # Create logo
            logo = logomaker.Logo(df_logo, shade_below=.5, fade_below=.5, font_name='DejaVu Sans')
            logo.ax.set_xlabel('Position')
            logo.ax.set_ylabel('Frequency')
            logo_file = os.path.join(logo_dir, f"cluster_{cluster_id}_logo.png")
            logo.fig.savefig(logo_file, dpi=150)
            plt.close(logo.fig)
            logger.info(f"[Kernel {k}] Sequence logo for cluster {cluster_id} saved to {logo_file}.")
    else:
        logger.info(f"[Kernel {k}] Not enough subsequences to cluster.")

# Run the extraction and clustering process for each kernel size
for k in kernel_sizes:
    extract_and_cluster_by_kernel_size(k)

logger.info("Motif extraction by kernel size complete. Check report/motifs_by_kernel for results.")
# === Post-Attribution Analysis End ===


================================================================================
FILE: src/pipelines/run_combined.py
================================================================================
# run_combined.py
import json
import os
from src.reporting.generate_report import generate_html_report

# 1. Run regression pipeline
os.system("python -m src.pipelines.run_regression")

# 2. Run classification pipeline
os.system("python -m src.pipelines.run_classification")

# 3. Run data exploration pipeline
os.system("python -m src.pipelines.run_data_exploration")

# Load saved metrics and plots
with open("report/regression_metrics.json", 'r') as f:
    regression_results = json.load(f)

with open("report/regression_plots.json", 'r') as f:
    regression_plots = json.load(f)

with open("report/classification_metrics.json", 'r') as f:
    classification_results = json.load(f)

with open("report/classification_plots.json", 'r') as f:
    classification_plots = json.load(f)

with open("report/data_exploration_plots.json", 'r') as f:
    data_exploration_plots = json.load(f)

# Combine regression and classification plots and also keep data exploration separate
# Here we do not prepend 'report/' to keep paths relative from index.html
all_plots = {}
all_plots.update(regression_plots)
all_plots.update(classification_plots)

generate_html_report(
    regression_results=regression_results,
    classification_results=classification_results,
    data_exploration_plots=data_exploration_plots,
    plots=all_plots,
    output_path="report/combined_report.html"
)

print("Combined report generated at: report/combined_report.html")


================================================================================
FILE: src/pipelines/run_classification.py
================================================================================
# run_classification.py
import os
import json
import logging
import numpy as np
from sklearn.model_selection import train_test_split
from src.config import FEATURE_DATA_CSV
from src.utils.file_utils import load_csv
from src.models.classical import (
    train_logistic_regression, predict_logistic_regression,
    train_random_forest_classifier, predict_random_forest_classifier
)
from src.evaluation.metrics import evaluate_classification
from src.evaluation.visualization import plot_distribution, plot_roc_curve

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting classification pipeline...")

df = load_csv(FEATURE_DATA_CSV)
logger.info("Features loaded successfully.")

y = (df['score'].values > 0.5).astype(int)
X = df.drop(columns=['chrom', 'start', 'end', 'score']).values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
logger.info(f"Data split into train ({len(X_train)}) and test ({len(X_test)}) samples.")

lr_class_model = train_logistic_regression(X_train, y_train)
y_pred_proba_lr = predict_logistic_regression(lr_class_model, X_test)
lr_class_metrics = evaluate_classification(y_test, y_pred_proba_lr)
logger.info("Logistic Regression trained and evaluated.")

best_auc = 0.0
best_n_estimators = 100
for n in [100, 200]:
    rf_model_candidate = train_random_forest_classifier(X_train, y_train, n_estimators=n, random_state=42)
    y_pred_proba_rf_cand = predict_random_forest_classifier(rf_model_candidate, X_test)
    candidate_metrics = evaluate_classification(y_test, y_pred_proba_rf_cand)
    if candidate_metrics['auc'] > best_auc:
        best_auc = candidate_metrics['auc']
        best_n_estimators = n

rf_class_model = train_random_forest_classifier(X_train, y_train, n_estimators=best_n_estimators, random_state=42)
y_pred_proba_rf = predict_random_forest_classifier(rf_class_model, X_test)
rf_class_metrics = evaluate_classification(y_test, y_pred_proba_rf)
logger.info(f"Random Forest Classifier trained and evaluated with n_estimators={best_n_estimators}.")

# Placeholder MLP model predictions
y_pred_proba_MLP = np.random.rand(len(y_test))
MLP_class_metrics = evaluate_classification(y_test, y_pred_proba_MLP)
logger.info("MLP classifier evaluated with placeholder predictions.")

y_pred_proba_ensemble = (y_pred_proba_lr + y_pred_proba_rf + y_pred_proba_MLP) / 3
ensemble_class_metrics = evaluate_classification(y_test, y_pred_proba_ensemble)
logger.info("Ensemble classification predictions evaluated.")

os.makedirs("report/classification_plots", exist_ok=True)
plot_distribution(y, "Distribution of Class Labels", "report/classification_plots/class_label_distribution.png")
plot_roc_curve(y_test, y_pred_proba_lr, "LR ROC Curve", "report/classification_plots/lr_roc.png")
plot_roc_curve(y_test, y_pred_proba_rf, "RF ROC Curve", "report/classification_plots/rf_roc.png")
plot_roc_curve(y_test, y_pred_proba_MLP, "MLP ROC Curve", "report/classification_plots/mlp_roc.png")
plot_roc_curve(y_test, y_pred_proba_ensemble, "Ensemble ROC Curve", "report/classification_plots/ensemble_roc.png")

classification_results = {
    "Logistic Regression": lr_class_metrics,
    "Random Forest": rf_class_metrics,
    "MLP": MLP_class_metrics,
    "Ensemble": ensemble_class_metrics
}

classification_plots = {
    "class_label_distribution": "classification_plots/class_label_distribution.png",
    "lr_roc": "classification_plots/lr_roc.png",
    "rf_roc": "classification_plots/rf_roc.png",
    "mlp_roc": "classification_plots/mlp_roc.png",
    "ensemble_roc": "classification_plots/ensemble_roc.png"
}

with open("report/classification_metrics.json", 'w') as f:
    json.dump(classification_results, f, indent=4)

with open("report/classification_plots.json", 'w') as f:
    json.dump(classification_plots, f, indent=4)

logger.info("Classification metrics and plots saved as JSON.")
logger.info("Classification pipeline completed.")

