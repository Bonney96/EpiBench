
================================================================================
FILE: report/classification_plots.json
================================================================================
{
    "class_label_distribution": "report/classification_plots/class_label_distribution.png",
    "lr_roc": "report/classification_plots/lr_roc.png",
    "rf_roc": "report/classification_plots/rf_roc.png",
    "cnn_roc": "report/classification_plots/cnn_roc.png",
    "ensemble_roc": "report/classification_plots/ensemble_roc.png"
}

================================================================================
FILE: report/classification_metrics.json
================================================================================
{
    "Logistic Regression": {
        "auc": 0.9368151632540122,
        "accuracy": 0.8605947955390335,
        "f1": 0.8598130841121495,
        "precision": 0.8949416342412452,
        "recall": 0.8273381294964028
    },
    "Random Forest": {
        "auc": 0.9223713337022689,
        "accuracy": 0.8568773234200744,
        "f1": 0.8617594254937163,
        "precision": 0.8602150537634409,
        "recall": 0.8633093525179856
    },
    "CNN": {
        "auc": 0.5048976203652462,
        "accuracy": 0.49256505576208176,
        "f1": 0.5009140767824497,
        "precision": 0.5092936802973977,
        "recall": 0.49280575539568344
    },
    "Ensemble": {
        "auc": 0.9043580520199225,
        "accuracy": 0.8345724907063197,
        "f1": 0.838475499092559,
        "precision": 0.8461538461538461,
        "recall": 0.8309352517985612
    }
}

================================================================================
FILE: report/regression_metrics.json
================================================================================
{
    "Linear Regression": {
        "mse": 0.13645962707760573,
        "r2": 0.23142911432665103,
        "pearson_corr": 0.4829654739041663
    },
    "Random Forest": {
        "mse": 0.0741476293632621,
        "r2": 0.5823841059019443,
        "pearson_corr": 0.7632906358420863
    },
    "CNN": {
        "mse": 0.1958589775629358,
        "r2": -0.10312120204618203,
        "pearson_corr": -0.03255154179762708
    },
    "Ensemble": {
        "mse": 0.10752252393948776,
        "r2": 0.3944092972860853,
        "pearson_corr": 0.6952129281141961
    }
}

================================================================================
FILE: report/regression_plots.json
================================================================================
{
    "score_distribution": "report/regression_plots/score_distribution.png",
    "lr_pred_vs_actual": "report/regression_plots/lr_pred_vs_actual.png",
    "rf_pred_vs_actual": "report/regression_plots/rf_pred_vs_actual.png",
    "cnn_pred_vs_actual": "report/regression_plots/cnn_pred_vs_actual.png",
    "ensemble_pred_vs_actual": "report/regression_plots/ensemble_pred_vs_actual.png"
}

================================================================================
FILE: snapshots/code_snapshot_20241216_012451.txt
================================================================================


================================================================================
FILE: snapshots/txt_snap.py
================================================================================
import os
from datetime import datetime

# Root directory containing the code
ROOT_DIR = "/storage1/fs1/dspencer/Active/spencerlab/abonney/EpiBench"

# File extensions to include in the snapshot
FILE_EXTENSIONS = ['.py', '.sh', '.R', '.ipynb', '.yml', '.yaml', '.json', '.txt', '.md']

def should_include(file_name):
    # Exclude macOS resource fork files that start with '._'
    if file_name.startswith("._"):
        return False
    # Include file if it has one of the allowed extensions
    return any(file_name.lower().endswith(ext) for ext in FILE_EXTENSIONS)

def create_text_snapshot(root_dir, extensions):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_name = f"code_snapshot_{timestamp}.txt"
    snapshot_path = os.path.join(os.getcwd(), snapshot_name)

    with open(snapshot_path, 'w', encoding='utf-8', errors='replace') as out_file:
        # Recursively walk the root directory
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if should_include(file):
                    full_path = os.path.join(root, file)
                    # Write a header for each file before its content
                    relative_path = os.path.relpath(full_path, start=root_dir)
                    out_file.write("\n" + "="*80 + "\n")
                    out_file.write(f"FILE: {relative_path}\n")
                    out_file.write("="*80 + "\n")

                    # Append file contents
                    try:
                        with open(full_path, 'r', encoding='utf-8', errors='replace') as f:
                            content = f.read()
                        out_file.write(content)
                        out_file.write("\n")  # Blank line after file content
                    except Exception as e:
                        out_file.write(f"\n[Error reading file: {e}]\n")

    print(f"Text snapshot created: {snapshot_path}")

if __name__ == "__main__":
    create_text_snapshot(ROOT_DIR, FILE_EXTENSIONS)


================================================================================
FILE: snapshots/snapshot.py
================================================================================
import os
import tarfile
from datetime import datetime

# Root directory containing the code
ROOT_DIR = "/storage1/fs1/dspencer/Active/spencerlab/abonney/EpiBench"

# File extensions to include in the snapshot
FILE_EXTENSIONS = ['.py', '.sh', '.R', '.ipynb', '.yml', '.yaml', '.json', '.txt', '.md']

def should_include(file_name):
    # Include file if it has one of the allowed extensions
    return any(file_name.lower().endswith(ext) for ext in FILE_EXTENSIONS)

def create_snapshot(root_dir, extensions):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_name = f"code_snapshot_{timestamp}.tar.gz"
    snapshot_path = os.path.join(os.getcwd(), snapshot_name)

    # Open the tar.gz file for writing
    with tarfile.open(snapshot_path, "w:gz") as tar:
        # Recursively walk the root directory
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if should_include(file):
                    full_path = os.path.join(root, file)
                    # Add file to the archive, preserving directory structure relative to ROOT_DIR
                    arcname = os.path.relpath(full_path, start=root_dir)
                    tar.add(full_path, arcname=arcname)

    print(f"Snapshot created: {snapshot_path}")

if __name__ == "__main__":
    create_snapshot(ROOT_DIR, FILE_EXTENSIONS)


================================================================================
FILE: snapshots/code_snapshot_20241215_222416.txt
================================================================================

================================================================================
FILE: txt_snap.py
================================================================================
import os
from datetime import datetime

# Root directory containing the code
ROOT_DIR = "/storage1/fs1/dspencer/Active/spencerlab/abonney/EpiBench"

# File extensions to include in the snapshot
FILE_EXTENSIONS = ['.py', '.sh', '.R', '.ipynb', '.yml', '.yaml', '.json', '.txt', '.md']

def should_include(file_name):
    # Exclude macOS resource fork files that start with '._'
    if file_name.startswith("._"):
        return False
    # Include file if it has one of the allowed extensions
    return any(file_name.lower().endswith(ext) for ext in FILE_EXTENSIONS)

def create_text_snapshot(root_dir, extensions):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_name = f"code_snapshot_{timestamp}.txt"
    snapshot_path = os.path.join(os.getcwd(), snapshot_name)

    with open(snapshot_path, 'w', encoding='utf-8', errors='replace') as out_file:
        # Recursively walk the root directory
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if should_include(file):
                    full_path = os.path.join(root, file)
                    # Write a header for each file before its content
                    relative_path = os.path.relpath(full_path, start=root_dir)
                    out_file.write("\n" + "="*80 + "\n")
                    out_file.write(f"FILE: {relative_path}\n")
                    out_file.write("="*80 + "\n")

                    # Append file contents
                    try:
                        with open(full_path, 'r', encoding='utf-8', errors='replace') as f:
                            content = f.read()
                        out_file.write(content)
                        out_file.write("\n")  # Blank line after file content
                    except Exception as e:
                        out_file.write(f"\n[Error reading file: {e}]\n")

    print(f"Text snapshot created: {snapshot_path}")

if __name__ == "__main__":
    create_text_snapshot(ROOT_DIR, FILE_EXTENSIONS)


================================================================================
FILE: snapshot.py
================================================================================
import os
import tarfile
from datetime import datetime

# Root directory containing the code
ROOT_DIR = "/storage1/fs1/dspencer/Active/spencerlab/abonney/EpiBench"

# File extensions to include in the snapshot
FILE_EXTENSIONS = ['.py', '.sh', '.R', '.ipynb', '.yml', '.yaml', '.json', '.txt', '.md']

def should_include(file_name):
    # Include file if it has one of the allowed extensions
    return any(file_name.lower().endswith(ext) for ext in FILE_EXTENSIONS)

def create_snapshot(root_dir, extensions):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_name = f"code_snapshot_{timestamp}.tar.gz"
    snapshot_path = os.path.join(os.getcwd(), snapshot_name)

    # Open the tar.gz file for writing
    with tarfile.open(snapshot_path, "w:gz") as tar:
        # Recursively walk the root directory
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if should_include(file):
                    full_path = os.path.join(root, file)
                    # Add file to the archive, preserving directory structure relative to ROOT_DIR
                    arcname = os.path.relpath(full_path, start=root_dir)
                    tar.add(full_path, arcname=arcname)

    print(f"Snapshot created: {snapshot_path}")

if __name__ == "__main__":
    create_snapshot(ROOT_DIR, FILE_EXTENSIONS)


================================================================================
FILE: code_snapshot_20241215_222416.txt
================================================================================


================================================================================
FILE: src/__init__.py
================================================================================
# src/__init__.py
# This top-level __init__ can import frequently used items to make them accessible directly from `src`.
# For example, import the config variables at the top level.

from .config import (
    RAW_BED_FILE,
    RAW_FASTA_FILE,
    PREPARED_DATA_CSV,
    FEATURE_DATA_CSV,
    RANDOM_SEED
)

# This way, I can do: from src import PREPARED_DATA_CSV


================================================================================
FILE: src/config.py
================================================================================
# config.py

##########################
# File Paths
##########################
RAW_BED_FILE = "data/raw/ND150826-CD34-wgbs.positive_control_regions.meth.bed"
RAW_FASTA_FILE = "data/raw/positive_control_sequences.fa"
PREPARED_DATA_CSV = "data/processed/positive_control_data_prepared.csv"
FEATURE_DATA_CSV = "data/processed/positive_control_features.csv"

##########################
# General Settings
##########################
RANDOM_SEED = 42  # For reproducibility


================================================================================
FILE: src/reporting/__init__.py
================================================================================
# src/reporting/__init__.py
from .generate_report import generate_html_report


================================================================================
FILE: src/reporting/generate_report.py
================================================================================
# generate_report.py

import os

def generate_html_report(regression_results, classification_results, plots, output_path):
    """
    regression_results: dict of model_name -> {mse, r2, pearson_corr}
    classification_results: dict of model_name -> {auc, accuracy, f1, precision, recall}
    plots: dict containing file paths of generated plots (keys like 'distribution', 'pred_vs_actual_lr', etc.)
    output_path: path to the final HTML file
    """
    
    # Create a simple HTML table for regression
    regression_table = "<table class='metric-table'><tr><th>Model</th><th>MSE</th><th>R²</th><th>Pearson Corr</th></tr>"
    for model, metrics in regression_results.items():
        regression_table += f"<tr><td>{model}</td><td>{metrics['mse']:.4f}</td><td>{metrics['r2']:.4f}</td><td>{metrics['pearson_corr']:.4f}</td></tr>"
    regression_table += "</table>"

    # Create a simple HTML table for classification
    classification_table = "<table class='metric-table'><tr><th>Model</th><th>AUC</th><th>Accuracy</th><th>F1</th><th>Precision</th><th>Recall</th></tr>"
    for model, metrics in classification_results.items():
        classification_table += f"<tr><td>{model}</td><td>{metrics['auc']:.4f}</td><td>{metrics['accuracy']:.4f}</td><td>{metrics['f1']:.4f}</td><td>{metrics['precision']:.4f}</td><td>{metrics['recall']:.4f}</td></tr>"
    classification_table += "</table>"

    # Insert images into the HTML report
    # Example: plot keys: 'score_distribution', 'lr_pred_vs_actual', 'rf_pred_vs_actual', 'cnn_pred_vs_actual', 'roc_lr'
    images_html = ""
    for plot_name, plot_path in plots.items():
        images_html += f"<h3>{plot_name.replace('_',' ').title()}</h3><img src='{plot_path}' alt='{plot_name}'>"

    html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Methylation Prediction Report</title>
<style>
body {{
    font-family: Arial, sans-serif;
    margin: 20px;
}}
h1 {{
    color: #333;
}}
.metric-table {{
    border-collapse: collapse;
    margin: 20px 0;
    width: 60%;
}}
.metric-table th, .metric-table td {{
    border: 1px solid #ccc;
    padding: 8px 12px;
    text-align: center;
}}
.metric-table th {{
    background-color: #f4f4f4;
}}
img {{
    max-width: 600px;
    height: auto;
    display: block;
    margin: 20px 0;
}}
</style>
</head>
<body>
<h1>Methylation Prediction Report</h1>
<p>This report compares multiple models for both regression and classification tasks.</p>

<h2>Regression Performance</h2>
{regression_table}

<h2>Classification Performance</h2>
{classification_table}

<h2>Plots</h2>
{images_html}

<p>The Random Forest approach often provides strong performance, as seen in both regression and classification results.</p>

</body>
</html>
"""

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w') as f:
        f.write(html_content)
    print(f"Report generated at: {output_path}")


================================================================================
FILE: src/evalutaion/__init__.py
================================================================================
# src/evaluation/__init__.py
from .metrics import evaluate_regression, evaluate_classification
from .visualization import (
    plot_distribution, plot_pred_vs_actual, plot_roc_curve, plot_precision_recall
)


================================================================================
FILE: src/evalutaion/visualization.py
================================================================================
# visualization.py

import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score


def plot_distribution(y, title, save_path):
    plt.figure(figsize=(6,4))
    sns.histplot(y, kde=True, color='blue', bins=20)
    plt.title(title)
    plt.xlabel("Value")
    plt.ylabel("Count")
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def plot_pred_vs_actual(y_true, y_pred, title, save_path):
    plt.figure(figsize=(6,6))
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([0,1],[0,1], 'r--')
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.title(title)
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def plot_roc_curve(y_true, y_pred_proba, title, save_path):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6,6))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC area = {roc_auc:.2f}')
    plt.plot([0,1],[0,1], 'r--')
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(title)
    plt.legend(loc="lower right")
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def _ensure_dir_exists(path):
    dir_name = os.path.dirname(path)
    if dir_name and not os.path.exists(dir_name):
        os.makedirs(dir_name)


================================================================================
FILE: src/evalutaion/metrics.py
================================================================================
# metrics.py

from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score
from scipy.stats import pearsonr
import numpy as np

def evaluate_regression(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    if np.std(y_pred) == 0 or np.std(y_true) == 0:
        corr = 0.0
    else:
        corr, _ = pearsonr(y_true, y_pred)
    return {
        'mse': mse,
        'r2': r2,
        'pearson_corr': corr
    }

def evaluate_classification(y_true, y_pred_proba, threshold=0.5):
    y_pred = (y_pred_proba >= threshold).astype(int)
    auc = roc_auc_score(y_true, y_pred_proba)
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    return {
        'auc': auc,
        'accuracy': acc,
        'f1': f1,
        'precision': prec,
        'recall': rec
    }


================================================================================
FILE: src/utils/__init__.py
================================================================================
# src/utils/__init__.py
# Re-export utilities for easy access.
from .file_utils import load_csv, save_csv
from .seq_utils import gc_content, cpg_density, kmer_counts


================================================================================
FILE: src/utils/seq_utils.py
================================================================================
# seq_utils.py
from itertools import product

def gc_content(seq):
    seq = seq.upper()
    gc_count = seq.count('G') + seq.count('C')
    return gc_count / len(seq) if len(seq) > 0 else 0

def cpg_density(seq):
    seq = seq.upper()
    cpg_count = seq.count("CG")
    return cpg_count / len(seq) if len(seq) > 0 else 0

def kmer_counts(seq, k=2):
    seq = seq.upper()
    counts = {}
    possible_kmers = [''.join(p) for p in product('ACGT', repeat=k)]
    for km in possible_kmers:
        counts[km] = 0
    
    for i in range(len(seq)-k+1):
        kmer = seq[i:i+k]
        if kmer in counts:
            counts[kmer] += 1
    
    length = len(seq)
    if length > 0:
        for km in counts:
            counts[km] = counts[km] / (length - k + 1)
    return counts


================================================================================
FILE: src/utils/file_utils.py
================================================================================
# file_utils.py
import pandas as pd
import os

def load_csv(path):
    """
    Load a CSV file into a pandas DataFrame.
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"File not found: {path}")
    return pd.read_csv(path)

def save_csv(df, path, index=False):
    """
    Save a pandas DataFrame to a CSV file.
    """
    directory = os.path.dirname(path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory)
    df.to_csv(path, index=index)
    print(f"Saved CSV to: {path}")


================================================================================
FILE: src/models/classical.py
================================================================================
# classical.py

import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

def train_linear_regression(X, y):
    model = LinearRegression()
    model.fit(X, y)
    return model

def predict_linear_regression(model, X):
    return model.predict(X)

def train_random_forest_regressor(X, y, n_estimators=100, random_state=42):
    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)
    model.fit(X, y)
    return model

def predict_random_forest_regressor(model, X):
    return model.predict(X)

def train_logistic_regression(X, y):
    model = LogisticRegression(solver='liblinear', random_state=42)
    model.fit(X, y)
    return model

def predict_logistic_regression(model, X):
    # return probabilities for classification tasks
    return model.predict_proba(X)[:, 1]

def train_random_forest_classifier(X, y, n_estimators=100, random_state=42):
    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)
    model.fit(X, y)
    return model

def predict_random_forest_classifier(model, X):
    return model.predict_proba(X)[:, 1]


================================================================================
FILE: src/models/__init__.py
================================================================================
# src/models/__init__.py
# Re-export model training functions and classes to access them easily.

from .classical import (
    train_linear_regression, predict_linear_regression,
    train_random_forest_regressor, predict_random_forest_regressor,
    train_logistic_regression, predict_logistic_regression,
    train_random_forest_classifier, predict_random_forest_classifier
)

from .deep_learning import (
    CNNRegressor, train_deep_model, predict_deep_model,
    RNNRegressor, TransformerRegressor
)

from .datasets import SequenceDataset


================================================================================
FILE: src/models/deep_learning.py
================================================================================
# deep_learning.py

import torch
import torch.nn as nn
import torch.optim as optim

# Example CNN model for regression
class CNNRegressor(nn.Module):
    def __init__(self, input_channels=4, seq_len=100):
        super(CNNRegressor, self).__init__()
        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=7, padding=3)
        self.bn1 = nn.BatchNorm1d(32)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=7, padding=3)
        self.bn2 = nn.BatchNorm1d(64)
        
        # Assume seq_len halved twice by pooling (seq_len/4)
        self.fc1 = nn.Linear((seq_len//4)*64, 64)
        self.fc2 = nn.Linear(64, 1)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.pool(x)
        
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.pool(x)
        
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

def train_deep_model(model, train_loader, val_loader, epochs=10, lr=0.001, device='cpu'):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    model.to(device)
    
    best_val_loss = float('inf')
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device, dtype=torch.float32).unsqueeze(1)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_train_loss = total_loss / len(train_loader)
        
        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device, dtype=torch.float32).unsqueeze(1)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
        avg_val_loss = val_loss / len(val_loader)
        
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            # Save model state if improving
            torch.save(model.state_dict(), "best_model.pth")
        
        print(f"Epoch {epoch+1}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}")

def predict_deep_model(model, test_loader, device='cpu'):
    model.to(device)
    model.eval()
    preds = []
    with torch.no_grad():
        for X_batch, _ in test_loader:
            X_batch = X_batch.to(device)
            outputs = model(X_batch)
            preds.extend(outputs.squeeze(1).cpu().tolist())
    return preds

class RNNRegressor(nn.Module):
    """A simple RNN-based model for regression."""
    def __init__(self, input_size=4, hidden_size=64, num_layers=1, seq_len=100):
        super(RNNRegressor, self).__init__()
        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        # x: (batch, channels, seq_len) -> we want (batch, seq_len, channels)
        x = x.permute(0, 2, 1)  # now (batch, seq_len, channels)
        _, (hn, _) = self.rnn(x)
        # hn: (num_layers, batch, hidden_size)
        # Take last hidden state of the last layer:
        out = self.fc(hn[-1])
        return out

class TransformerRegressor(nn.Module):
    """A simple Transformer-based model for regression."""
    def __init__(self, input_dim=4, d_model=64, nhead=4, num_layers=2, seq_len=100):
        super(TransformerRegressor, self).__init__()
        self.embedding = nn.Linear(input_dim, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(d_model, 1)
        
    def forward(self, x):
        # x: (batch, channels, seq_len)
        # Transform to (seq_len, batch, d_model)
        x = x.permute(2, 0, 1)  
        x = self.embedding(x)  
        # pass through transformer
        out = self.transformer_encoder(x)  # (seq_len, batch, d_model)
        # Take the mean across seq_len
        out = out.mean(dim=0)  # (batch, d_model)
        out = self.fc(out)
        return out

================================================================================
FILE: src/models/datasets.py
================================================================================
# datasets.py
import torch
from torch.utils.data import Dataset
import numpy as np

class SequenceDataset(Dataset):
    """
    A PyTorch Dataset for sequences.
    """
    def __init__(self, df, seq_length, target_col='score', classification=False, threshold=0.5):
        """
        df: DataFrame containing 'sequence' and 'score' columns, plus features if needed.
        seq_length: Fixed length of sequences.
        classification: If True, prepare binary classification targets.
        threshold: Threshold for binary classification.
        """
        self.df = df.reset_index(drop=True)
        self.seq_length = seq_length
        self.classification = classification
        self.threshold = threshold

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        seq = row['sequence']
        X_seq = self.one_hot_encode(seq, self.seq_length)
        y = row['score']
        if self.classification:
            y = 1 if y > self.threshold else 0
        return X_seq, y

    def one_hot_encode(self, seq, fixed_len):
        seq = seq.upper()
        mapping = {'A':0, 'C':1, 'G':2, 'T':3}
        arr = np.zeros((4, fixed_len), dtype=np.float32)
        seq = seq[:fixed_len]
        if len(seq) < fixed_len:
            seq = seq + 'A'*(fixed_len - len(seq))
        for i, base in enumerate(seq):
            arr[mapping.get(base, 0), i] = 1.0
        return arr


================================================================================
FILE: src/data_preparation/prepare_data.py
================================================================================
import sys
from Bio import SeqIO

# Update these paths as needed
bed_file = "/storage1/fs1/dspencer/Active/spencerlab/abonney/regions/simple_project/ND150826-CD34-wgbs.positive_control_regions.meth.bed"
fasta_file = "/storage1/fs1/dspencer/Active/spencerlab/abonney/regions/simple_project/positive_control_sequences.fa"
output_csv = "positive_control_data.csv"

# Read BED data
# Based on the data you showed, columns are:
# 0: chrom
# 1: start
# 2: end
# 3: coverage (not needed)
# 4: score (between 0 and 1)
regions = []
with open(bed_file, 'r') as bf:
    for line in bf:
        if line.strip():
            parts = line.strip().split('\t')
            chrom = parts[0]
            start = parts[1]
            end = parts[2]
            # coverage = parts[3]  # If you need it, but not required now
            score = float(parts[4])
            regions.append((chrom, start, end, score))

# Create a dictionary keyed by region string (chr:start-end) to score
score_dict = {}
for (chrom, start, end, score) in regions:
    region_key = f"{chrom}:{start}-{end}"
    score_dict[region_key] = score

# Parse FASTA and match sequences to scores
with open(output_csv, 'w') as out:
    out.write("chrom,start,end,score,sequence\n")
    for record in SeqIO.parse(fasta_file, "fasta"):
        # record.id should be something like: chr1:10000-10100
        region = record.id
        seq = str(record.seq)
        chrom = region.split(':')[0]
        coords = region.split(':')[1]
        start = coords.split('-')[0]
        end = coords.split('-')[1]
        score = score_dict.get(region, "unknown")
        out.write(f"{chrom},{start},{end},{score},{seq}\n")


================================================================================
FILE: src/data_preparation/__init__.py
================================================================================
# src/data_preparation/__init__.py

from .prepare_data import main as prepare_data_main
from .feature_engineering import main as feature_engineering_main

================================================================================
FILE: src/data_preparation/feature_engineering.py
================================================================================
# feature_engineering.py
import pandas as pd
from src.config import PREPARED_DATA_CSV, FEATURE_DATA_CSV
from src.utils.seq_utils import gc_content, cpg_density, kmer_counts
from itertools import product

df = pd.read_csv(PREPARED_DATA_CSV)

all_features = []
for _, row in df.iterrows():
    seq = row['sequence']
    features = {
        'chrom': row['chrom'],
        'start': row['start'],
        'end': row['end'],
        'score': row['score'],
        'seq_length': len(seq),
        'gc_content': gc_content(seq),
        'cpg_density': cpg_density(seq)
    }
    
    # Add 2-mer frequencies
    k2_counts = kmer_counts(seq, k=2)
    for kmer, val in k2_counts.items():
        features[f"k2_{kmer}"] = val
    
    # Add 3-mer frequencies
    k3_counts = kmer_counts(seq, k=3)
    for kmer, val in k3_counts.items():
        features[f"k3_{kmer}"] = val
    
    all_features.append(features)

feature_df = pd.DataFrame(all_features)
feature_df.to_csv(FEATURE_DATA_CSV, index=False)


================================================================================
FILE: src/pipelines/run_regression.py
================================================================================
# run_regression.py
import os
import json
import logging
import numpy as np
from sklearn.model_selection import train_test_split
from src.config import FEATURE_DATA_CSV
from src.utils.file_utils import load_csv
from src.models.classical import (
    train_linear_regression, predict_linear_regression,
    train_random_forest_regressor, predict_random_forest_regressor
)
from src.models.deep_learning import CNNRegressor, train_deep_model, predict_deep_model
from src.evaluation.metrics import evaluate_regression
from src.evaluation.visualization import plot_distribution, plot_pred_vs_actual
import torch
from torch.utils.data import TensorDataset, DataLoader

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting regression pipeline...")

# Load feature data
df = load_csv(FEATURE_DATA_CSV)
logger.info("Features loaded successfully.")

# Extract target and features
y = df['score'].values
X = df.drop(columns=['chrom', 'start', 'end', 'score']).values

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
logger.info(f"Data split into train ({len(X_train)}) and test ({len(X_test)}) samples.")

# Train Linear Regression
lr_model = train_linear_regression(X_train, y_train)
y_pred_lr = predict_linear_regression(lr_model, X_test)
lr_metrics = evaluate_regression(y_test, y_pred_lr)
logger.info("Linear Regression trained and evaluated.")

# Train Random Forest Regressor with simple hyperparameter tuning
best_mse = float('inf')
best_n_estimators = 100
for n in [100, 200, 300]:
    rf_model_candidate = train_random_forest_regressor(X_train, y_train, n_estimators=n, random_state=42)
    y_pred_rf_candidate = predict_random_forest_regressor(rf_model_candidate, X_test)
    candidate_metrics = evaluate_regression(y_test, y_pred_rf_candidate)
    if candidate_metrics['mse'] < best_mse:
        best_mse = candidate_metrics['mse']
        best_n_estimators = n

rf_model = train_random_forest_regressor(X_train, y_train, n_estimators=best_n_estimators, random_state=42)
y_pred_rf = predict_random_forest_regressor(rf_model, X_test)
rf_metrics = evaluate_regression(y_test, y_pred_rf)
logger.info(f"Random Forest trained and evaluated with n_estimators={best_n_estimators}.")

# CNN for Regression (assuming sequences are pre-encoded or use a real dataset)
seq_length = 100
X_train_seq = torch.randn(len(X_train), 4, seq_length)  # Placeholder
X_test_seq = torch.randn(len(X_test), 4, seq_length)
train_dataset = TensorDataset(X_train_seq, torch.tensor(y_train, dtype=torch.float32))
test_dataset = TensorDataset(X_test_seq, torch.tensor(y_test, dtype=torch.float32))

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

cnn_model = CNNRegressor(input_channels=4, seq_len=seq_length)
train_deep_model(cnn_model, train_loader, val_loader, epochs=5, lr=0.001, device='cpu')
y_pred_cnn = predict_deep_model(cnn_model, test_loader, device='cpu')
cnn_metrics = evaluate_regression(y_test, np.array(y_pred_cnn))
logger.info("CNN model trained and evaluated.")

# Ensemble predictions
y_pred_ensemble = (y_pred_lr + y_pred_rf + y_pred_cnn) / 3
ensemble_metrics = evaluate_regression(y_test, y_pred_ensemble)
logger.info("Ensemble predictions evaluated.")

# Generate plots
os.makedirs("report/regression_plots", exist_ok=True)
plot_distribution(y, "Distribution of Scores", "report/regression_plots/score_distribution.png")
plot_pred_vs_actual(y_test, y_pred_lr, "Linear Regression Predictions", "report/regression_plots/lr_pred_vs_actual.png")
plot_pred_vs_actual(y_test, y_pred_rf, "Random Forest Predictions", "report/regression_plots/rf_pred_vs_actual.png")
plot_pred_vs_actual(y_test, y_pred_cnn, "CNN Predictions", "report/regression_plots/cnn_pred_vs_actual.png")
plot_pred_vs_actual(y_test, y_pred_ensemble, "Ensemble Predictions", "report/regression_plots/ensemble_pred_vs_actual.png")

# Save metrics and plots as JSON for combined report
regression_results = {
    "Linear Regression": lr_metrics,
    "Random Forest": rf_metrics,
    "CNN": cnn_metrics,
    "Ensemble": ensemble_metrics
}

plots = {
    "score_distribution": "report/regression_plots/score_distribution.png",
    "lr_pred_vs_actual": "report/regression_plots/lr_pred_vs_actual.png",
    "rf_pred_vs_actual": "report/regression_plots/rf_pred_vs_actual.png",
    "cnn_pred_vs_actual": "report/regression_plots/cnn_pred_vs_actual.png",
    "ensemble_pred_vs_actual": "report/regression_plots/ensemble_pred_vs_actual.png"
}

with open("report/regression_metrics.json", 'w') as f:
    json.dump(regression_results, f, indent=4)

with open("report/regression_plots.json", 'w') as f:
    json.dump(plots, f, indent=4)

logger.info("Regression metrics and plots saved as JSON.")
logger.info("Regression pipeline completed.")


================================================================================
FILE: src/pipelines/run_combined.py
================================================================================
# run_combined.py
import json
import os
from src.reporting.generate_report import generate_html_report

# Assume `run_regression.py` and `run_classification.py` have been updated 
# to save their metrics and plot paths to JSON files for easier integration.

# 1. Run regression pipeline
os.system("python src/pipelines/run_regression.py")

# 2. Run classification pipeline
os.system("python src/pipelines/run_classification.py")

# 3. Load saved metrics and plots
with open("report/regression_metrics.json", 'r') as f:
    regression_results = json.load(f)

with open("report/regression_plots.json", 'r') as f:
    regression_plots = json.load(f)

with open("report/classification_metrics.json", 'r') as f:
    classification_results = json.load(f)

with open("report/classification_plots.json", 'r') as f:
    classification_plots = json.load(f)

# Combine plots (if you wish to display all)
all_plots = {**regression_plots, **classification_plots}

# Generate a combined report
generate_html_report(
    regression_results=regression_results,
    classification_results=classification_results,
    plots=all_plots,
    output_path="report/combined_report.html"
)

print("Combined report generated at: report/combined_report.html")


================================================================================
FILE: src/pipelines/run_classification.py
================================================================================
# run_classification.py
import os
import json
import logging
import numpy as np
from sklearn.model_selection import train_test_split
from src.config import FEATURE_DATA_CSV
from src.utils.file_utils import load_csv
from src.models.classical import (
    train_logistic_regression, predict_logistic_regression,
    train_random_forest_classifier, predict_random_forest_classifier
)
from src.evaluation.metrics import evaluate_classification
from src.evaluation.visualization import plot_distribution, plot_roc_curve
import torch
from torch.utils.data import TensorDataset, DataLoader

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting classification pipeline...")

# Load feature data
df = load_csv(FEATURE_DATA_CSV)
logger.info("Features loaded successfully.")

# Binary classification target
y = (df['score'].values > 0.5).astype(int)
X = df.drop(columns=['chrom', 'start', 'end', 'score']).values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
logger.info(f"Data split into train ({len(X_train)}) and test ({len(X_test)}) samples.")

# Logistic Regression
lr_class_model = train_logistic_regression(X_train, y_train)
y_pred_proba_lr = predict_logistic_regression(lr_class_model, X_test)
lr_class_metrics = evaluate_classification(y_test, y_pred_proba_lr)
logger.info("Logistic Regression trained and evaluated.")

# Random Forest Classifier with simple tuning
best_auc = 0.0
best_n_estimators = 100
for n in [100, 200]:
    rf_model_candidate = train_random_forest_classifier(X_train, y_train, n_estimators=n, random_state=42)
    y_pred_proba_rf_cand = predict_random_forest_classifier(rf_model_candidate, X_test)
    candidate_metrics = evaluate_classification(y_test, y_pred_proba_rf_cand)
    if candidate_metrics['auc'] > best_auc:
        best_auc = candidate_metrics['auc']
        best_n_estimators = n

rf_class_model = train_random_forest_classifier(X_train, y_train, n_estimators=best_n_estimators, random_state=42)
y_pred_proba_rf = predict_random_forest_classifier(rf_class_model, X_test)
rf_class_metrics = evaluate_classification(y_test, y_pred_proba_rf)
logger.info(f"Random Forest Classifier trained and evaluated with n_estimators={best_n_estimators}.")

# CNN Classifier placeholder (assume a CNNClassifier model)
# For demonstration, use random predictions
y_pred_proba_cnn = np.random.rand(len(y_test))
cnn_class_metrics = evaluate_classification(y_test, y_pred_proba_cnn)
logger.info("CNN classifier evaluated with placeholder predictions.")

# Ensemble
y_pred_proba_ensemble = (y_pred_proba_lr + y_pred_proba_rf + y_pred_proba_cnn) / 3
ensemble_class_metrics = evaluate_classification(y_test, y_pred_proba_ensemble)
logger.info("Ensemble classification predictions evaluated.")

os.makedirs("report/classification_plots", exist_ok=True)
plot_distribution(y, "Distribution of Class Labels", "report/classification_plots/class_label_distribution.png")
plot_roc_curve(y_test, y_pred_proba_lr, "LR ROC Curve", "report/classification_plots/lr_roc.png")
plot_roc_curve(y_test, y_pred_proba_rf, "RF ROC Curve", "report/classification_plots/rf_roc.png")
plot_roc_curve(y_test, y_pred_proba_cnn, "CNN ROC Curve", "report/classification_plots/cnn_roc.png")
plot_roc_curve(y_test, y_pred_proba_ensemble, "Ensemble ROC Curve", "report/classification_plots/ensemble_roc.png")

classification_results = {
    "Logistic Regression": lr_class_metrics,
    "Random Forest": rf_class_metrics,
    "CNN": cnn_class_metrics,
    "Ensemble": ensemble_class_metrics
}

classification_plots = {
    "class_label_distribution": "report/classification_plots/class_label_distribution.png",
    "lr_roc": "report/classification_plots/lr_roc.png",
    "rf_roc": "report/classification_plots/rf_roc.png",
    "cnn_roc": "report/classification_plots/cnn_roc.png",
    "ensemble_roc": "report/classification_plots/ensemble_roc.png"
}

with open("report/classification_metrics.json", 'w') as f:
    json.dump(classification_results, f, indent=4)

with open("report/classification_plots.json", 'w') as f:
    json.dump(classification_plots, f, indent=4)

logger.info("Classification metrics and plots saved as JSON.")
logger.info("Classification pipeline completed.")



================================================================================
FILE: src/__init__.py
================================================================================
# src/__init__.py
# This top-level __init__ can import frequently used items to make them accessible directly from `src`.
# For example, import the config variables at the top level.

from .config import (
    RAW_BED_FILE,
    RAW_FASTA_FILE,
    PREPARED_DATA_CSV,
    FEATURE_DATA_CSV,
    RANDOM_SEED
)

# This way, I can do: from src import PREPARED_DATA_CSV


================================================================================
FILE: src/config.py
================================================================================
# config.py

##########################
# File Paths
##########################
RAW_BED_FILE = "data/raw/ND150826-CD34-wgbs.positive_control_regions.meth.bed"
RAW_FASTA_FILE = "data/raw/positive_control_sequences.fa"
PREPARED_DATA_CSV = "data/processed/positive_control_data_prepared.csv"
FEATURE_DATA_CSV = "data/processed/positive_control_features.csv"

##########################
# General Settings
##########################
RANDOM_SEED = 42  # For reproducibility


================================================================================
FILE: src/reporting/__init__.py
================================================================================
# src/reporting/__init__.py
from .generate_report import generate_html_report


================================================================================
FILE: src/reporting/generate_report.py
================================================================================
# generate_report.py

import os

def generate_html_report(regression_results, classification_results, plots, output_path):
    """
    regression_results: dict of model_name -> {mse, r2, pearson_corr}
    classification_results: dict of model_name -> {auc, accuracy, f1, precision, recall}
    plots: dict containing file paths of generated plots (keys like 'distribution', 'pred_vs_actual_lr', etc.)
    output_path: path to the final HTML file
    """
    
    # Create a simple HTML table for regression
    regression_table = "<table class='metric-table'><tr><th>Model</th><th>MSE</th><th>R²</th><th>Pearson Corr</th></tr>"
    for model, metrics in regression_results.items():
        regression_table += f"<tr><td>{model}</td><td>{metrics['mse']:.4f}</td><td>{metrics['r2']:.4f}</td><td>{metrics['pearson_corr']:.4f}</td></tr>"
    regression_table += "</table>"

    # Create a simple HTML table for classification
    classification_table = "<table class='metric-table'><tr><th>Model</th><th>AUC</th><th>Accuracy</th><th>F1</th><th>Precision</th><th>Recall</th></tr>"
    for model, metrics in classification_results.items():
        classification_table += f"<tr><td>{model}</td><td>{metrics['auc']:.4f}</td><td>{metrics['accuracy']:.4f}</td><td>{metrics['f1']:.4f}</td><td>{metrics['precision']:.4f}</td><td>{metrics['recall']:.4f}</td></tr>"
    classification_table += "</table>"

    # Insert images into the HTML report
    # Example: plot keys: 'score_distribution', 'lr_pred_vs_actual', 'rf_pred_vs_actual', 'cnn_pred_vs_actual', 'roc_lr'
    images_html = ""
    for plot_name, plot_path in plots.items():
        images_html += f"<h3>{plot_name.replace('_',' ').title()}</h3><img src='{plot_path}' alt='{plot_name}'>"

    html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Methylation Prediction Report</title>
<style>
body {{
    font-family: Arial, sans-serif;
    margin: 20px;
}}
h1 {{
    color: #333;
}}
.metric-table {{
    border-collapse: collapse;
    margin: 20px 0;
    width: 60%;
}}
.metric-table th, .metric-table td {{
    border: 1px solid #ccc;
    padding: 8px 12px;
    text-align: center;
}}
.metric-table th {{
    background-color: #f4f4f4;
}}
img {{
    max-width: 600px;
    height: auto;
    display: block;
    margin: 20px 0;
}}
</style>
</head>
<body>
<h1>Methylation Prediction Report</h1>
<p>This report compares multiple models for both regression and classification tasks.</p>

<h2>Regression Performance</h2>
{regression_table}

<h2>Classification Performance</h2>
{classification_table}

<h2>Plots</h2>
{images_html}

<p>The Random Forest approach often provides strong performance, as seen in both regression and classification results.</p>

</body>
</html>
"""

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w') as f:
        f.write(html_content)
    print(f"Report generated at: {output_path}")


================================================================================
FILE: src/utils/__init__.py
================================================================================
# src/utils/__init__.py
# Re-export utilities for easy access.
from .file_utils import load_csv, save_csv
from .seq_utils import gc_content, cpg_density, kmer_counts


================================================================================
FILE: src/utils/seq_utils.py
================================================================================
# seq_utils.py
from itertools import product

def gc_content(seq):
    seq = seq.upper()
    gc_count = seq.count('G') + seq.count('C')
    return gc_count / len(seq) if len(seq) > 0 else 0

def cpg_density(seq):
    seq = seq.upper()
    cpg_count = seq.count("CG")
    return cpg_count / len(seq) if len(seq) > 0 else 0

def kmer_counts(seq, k=2):
    seq = seq.upper()
    counts = {}
    possible_kmers = [''.join(p) for p in product('ACGT', repeat=k)]
    for km in possible_kmers:
        counts[km] = 0
    
    for i in range(len(seq)-k+1):
        kmer = seq[i:i+k]
        if kmer in counts:
            counts[kmer] += 1
    
    length = len(seq)
    if length > 0:
        for km in counts:
            counts[km] = counts[km] / (length - k + 1)
    return counts


================================================================================
FILE: src/utils/file_utils.py
================================================================================
# file_utils.py
import pandas as pd
import os

def load_csv(path):
    """
    Load a CSV file into a pandas DataFrame.
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"File not found: {path}")
    return pd.read_csv(path)

def save_csv(df, path, index=False):
    """
    Save a pandas DataFrame to a CSV file.
    """
    directory = os.path.dirname(path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory)
    df.to_csv(path, index=index)
    print(f"Saved CSV to: {path}")


================================================================================
FILE: src/models/classical.py
================================================================================
# classical.py

import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

def train_linear_regression(X, y):
    model = LinearRegression()
    model.fit(X, y)
    return model

def predict_linear_regression(model, X):
    return model.predict(X)

def train_random_forest_regressor(X, y, n_estimators=100, random_state=42):
    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)
    model.fit(X, y)
    return model

def predict_random_forest_regressor(model, X):
    return model.predict(X)

def train_logistic_regression(X, y):
    model = LogisticRegression(solver='liblinear', random_state=42)
    model.fit(X, y)
    return model

def predict_logistic_regression(model, X):
    # return probabilities for classification tasks
    return model.predict_proba(X)[:, 1]

def train_random_forest_classifier(X, y, n_estimators=100, random_state=42):
    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)
    model.fit(X, y)
    return model

def predict_random_forest_classifier(model, X):
    return model.predict_proba(X)[:, 1]


================================================================================
FILE: src/models/__init__.py
================================================================================
# src/models/__init__.py
# Re-export model training functions and classes to access them easily.

from .classical import (
    train_linear_regression, predict_linear_regression,
    train_random_forest_regressor, predict_random_forest_regressor,
    train_logistic_regression, predict_logistic_regression,
    train_random_forest_classifier, predict_random_forest_classifier
)

from .deep_learning import (
    CNNRegressor, train_deep_model, predict_deep_model,
    RNNRegressor, TransformerRegressor
)

from .datasets import SequenceDataset


================================================================================
FILE: src/models/deep_learning.py
================================================================================
# deep_learning.py

import torch
import torch.nn as nn
import torch.optim as optim

# Example CNN model for regression
class CNNRegressor(nn.Module):
    def __init__(self, input_channels=4, seq_len=100):
        super(CNNRegressor, self).__init__()
        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=7, padding=3)
        self.bn1 = nn.BatchNorm1d(32)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=7, padding=3)
        self.bn2 = nn.BatchNorm1d(64)
        
        # Assume seq_len halved twice by pooling (seq_len/4)
        self.fc1 = nn.Linear((seq_len//4)*64, 64)
        self.fc2 = nn.Linear(64, 1)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.pool(x)
        
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.pool(x)
        
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

def train_deep_model(model, train_loader, val_loader, epochs=10, lr=0.001, device='cpu'):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    model.to(device)
    
    best_val_loss = float('inf')
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device, dtype=torch.float32).unsqueeze(1)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_train_loss = total_loss / len(train_loader)
        
        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device, dtype=torch.float32).unsqueeze(1)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
        avg_val_loss = val_loss / len(val_loader)
        
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            # Save model state if improving
            torch.save(model.state_dict(), "best_model.pth")
        
        print(f"Epoch {epoch+1}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}")

def predict_deep_model(model, test_loader, device='cpu'):
    model.to(device)
    model.eval()
    preds = []
    with torch.no_grad():
        for X_batch, _ in test_loader:
            X_batch = X_batch.to(device)
            outputs = model(X_batch)
            preds.extend(outputs.squeeze(1).cpu().tolist())
    return preds

class RNNRegressor(nn.Module):
    """A simple RNN-based model for regression."""
    def __init__(self, input_size=4, hidden_size=64, num_layers=1, seq_len=100):
        super(RNNRegressor, self).__init__()
        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        # x: (batch, channels, seq_len) -> we want (batch, seq_len, channels)
        x = x.permute(0, 2, 1)  # now (batch, seq_len, channels)
        _, (hn, _) = self.rnn(x)
        # hn: (num_layers, batch, hidden_size)
        # Take last hidden state of the last layer:
        out = self.fc(hn[-1])
        return out

class TransformerRegressor(nn.Module):
    """A simple Transformer-based model for regression."""
    def __init__(self, input_dim=4, d_model=64, nhead=4, num_layers=2, seq_len=100):
        super(TransformerRegressor, self).__init__()
        self.embedding = nn.Linear(input_dim, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(d_model, 1)
        
    def forward(self, x):
        # x: (batch, channels, seq_len)
        # Transform to (seq_len, batch, d_model)
        x = x.permute(2, 0, 1)  
        x = self.embedding(x)  
        # pass through transformer
        out = self.transformer_encoder(x)  # (seq_len, batch, d_model)
        # Take the mean across seq_len
        out = out.mean(dim=0)  # (batch, d_model)
        out = self.fc(out)
        return out

================================================================================
FILE: src/models/datasets.py
================================================================================
# datasets.py
import torch
from torch.utils.data import Dataset
import numpy as np

class SequenceDataset(Dataset):
    """
    A PyTorch Dataset for sequences.
    """
    def __init__(self, df, seq_length, target_col='score', classification=False, threshold=0.5):
        """
        df: DataFrame containing 'sequence' and 'score' columns, plus features if needed.
        seq_length: Fixed length of sequences.
        classification: If True, prepare binary classification targets.
        threshold: Threshold for binary classification.
        """
        self.df = df.reset_index(drop=True)
        self.seq_length = seq_length
        self.classification = classification
        self.threshold = threshold

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        seq = row['sequence']
        X_seq = self.one_hot_encode(seq, self.seq_length)
        y = row['score']
        if self.classification:
            y = 1 if y > self.threshold else 0
        return X_seq, y

    def one_hot_encode(self, seq, fixed_len):
        seq = seq.upper()
        mapping = {'A':0, 'C':1, 'G':2, 'T':3}
        arr = np.zeros((4, fixed_len), dtype=np.float32)
        seq = seq[:fixed_len]
        if len(seq) < fixed_len:
            seq = seq + 'A'*(fixed_len - len(seq))
        for i, base in enumerate(seq):
            arr[mapping.get(base, 0), i] = 1.0
        return arr


================================================================================
FILE: src/data_preparation/prepare_data.py
================================================================================
import sys
from Bio import SeqIO

# Update these paths as needed
bed_file = "/storage1/fs1/dspencer/Active/spencerlab/abonney/regions/simple_project/ND150826-CD34-wgbs.positive_control_regions.meth.bed"
fasta_file = "/storage1/fs1/dspencer/Active/spencerlab/abonney/regions/simple_project/positive_control_sequences.fa"
output_csv = "positive_control_data.csv"

# Read BED data
# Based on the data you showed, columns are:
# 0: chrom
# 1: start
# 2: end
# 3: coverage (not needed)
# 4: score (between 0 and 1)
regions = []
with open(bed_file, 'r') as bf:
    for line in bf:
        if line.strip():
            parts = line.strip().split('\t')
            chrom = parts[0]
            start = parts[1]
            end = parts[2]
            # coverage = parts[3]  # If you need it, but not required now
            score = float(parts[4])
            regions.append((chrom, start, end, score))

# Create a dictionary keyed by region string (chr:start-end) to score
score_dict = {}
for (chrom, start, end, score) in regions:
    region_key = f"{chrom}:{start}-{end}"
    score_dict[region_key] = score

# Parse FASTA and match sequences to scores
with open(output_csv, 'w') as out:
    out.write("chrom,start,end,score,sequence\n")
    for record in SeqIO.parse(fasta_file, "fasta"):
        # record.id should be something like: chr1:10000-10100
        region = record.id
        seq = str(record.seq)
        chrom = region.split(':')[0]
        coords = region.split(':')[1]
        start = coords.split('-')[0]
        end = coords.split('-')[1]
        score = score_dict.get(region, "unknown")
        out.write(f"{chrom},{start},{end},{score},{seq}\n")


================================================================================
FILE: src/data_preparation/__init__.py
================================================================================
# src/data_preparation/__init__.py

from .prepare_data import main as prepare_data_main
from .feature_engineering import main as feature_engineering_main

================================================================================
FILE: src/data_preparation/feature_engineering.py
================================================================================
# feature_engineering.py
import pandas as pd
from src.config import PREPARED_DATA_CSV, FEATURE_DATA_CSV
from src.utils.seq_utils import gc_content, cpg_density, kmer_counts
from itertools import product

df = pd.read_csv(PREPARED_DATA_CSV)

all_features = []
for _, row in df.iterrows():
    seq = row['sequence']
    features = {
        'chrom': row['chrom'],
        'start': row['start'],
        'end': row['end'],
        'score': row['score'],
        'seq_length': len(seq),
        'gc_content': gc_content(seq),
        'cpg_density': cpg_density(seq)
    }
    
    # Add 2-mer frequencies
    k2_counts = kmer_counts(seq, k=2)
    for kmer, val in k2_counts.items():
        features[f"k2_{kmer}"] = val
    
    # Add 3-mer frequencies
    k3_counts = kmer_counts(seq, k=3)
    for kmer, val in k3_counts.items():
        features[f"k3_{kmer}"] = val
    
    all_features.append(features)

feature_df = pd.DataFrame(all_features)
feature_df.to_csv(FEATURE_DATA_CSV, index=False)


================================================================================
FILE: src/evaluation/__init__.py
================================================================================
# src/evaluation/__init__.py
from .metrics import evaluate_regression, evaluate_classification
from .visualization import (
    plot_distribution, plot_pred_vs_actual, plot_roc_curve
)


================================================================================
FILE: src/evaluation/visualization.py
================================================================================
# visualization.py

import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score


def plot_distribution(y, title, save_path):
    plt.figure(figsize=(6,4))
    sns.histplot(y, kde=True, color='blue', bins=20)
    plt.title(title)
    plt.xlabel("Value")
    plt.ylabel("Count")
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def plot_pred_vs_actual(y_true, y_pred, title, save_path):
    plt.figure(figsize=(6,6))
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([0,1],[0,1], 'r--')
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.title(title)
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def plot_roc_curve(y_true, y_pred_proba, title, save_path):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6,6))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC area = {roc_auc:.2f}')
    plt.plot([0,1],[0,1], 'r--')
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(title)
    plt.legend(loc="lower right")
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def _ensure_dir_exists(path):
    dir_name = os.path.dirname(path)
    if dir_name and not os.path.exists(dir_name):
        os.makedirs(dir_name)


================================================================================
FILE: src/evaluation/metrics.py
================================================================================
# metrics.py

from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score
from scipy.stats import pearsonr
import numpy as np

def evaluate_regression(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    if np.std(y_pred) == 0 or np.std(y_true) == 0:
        corr = 0.0
    else:
        corr, _ = pearsonr(y_true, y_pred)
    return {
        'mse': mse,
        'r2': r2,
        'pearson_corr': corr
    }

def evaluate_classification(y_true, y_pred_proba, threshold=0.5):
    y_pred = (y_pred_proba >= threshold).astype(int)
    auc = roc_auc_score(y_true, y_pred_proba)
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    return {
        'auc': auc,
        'accuracy': acc,
        'f1': f1,
        'precision': prec,
        'recall': rec
    }


================================================================================
FILE: src/pipelines/run_regression.py
================================================================================
# run_regression.py
import os
import json
import logging
import numpy as np
from sklearn.model_selection import train_test_split
from src.config import FEATURE_DATA_CSV
from src.utils.file_utils import load_csv
from src.models.classical import (
    train_linear_regression, predict_linear_regression,
    train_random_forest_regressor, predict_random_forest_regressor
)
from src.models.deep_learning import CNNRegressor, train_deep_model, predict_deep_model
from src.evaluation.metrics import evaluate_regression
from src.evaluation.visualization import plot_distribution, plot_pred_vs_actual
import torch
from torch.utils.data import TensorDataset, DataLoader

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting regression pipeline...")

# Load feature data
df = load_csv(FEATURE_DATA_CSV)
logger.info("Features loaded successfully.")

# Extract target and features
y = df['score'].values
X = df.drop(columns=['chrom', 'start', 'end', 'score']).values

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
logger.info(f"Data split into train ({len(X_train)}) and test ({len(X_test)}) samples.")

# Train Linear Regression
lr_model = train_linear_regression(X_train, y_train)
y_pred_lr = predict_linear_regression(lr_model, X_test)
lr_metrics = evaluate_regression(y_test, y_pred_lr)
logger.info("Linear Regression trained and evaluated.")

# Train Random Forest Regressor with simple hyperparameter tuning
best_mse = float('inf')
best_n_estimators = 100
for n in [100, 200, 300]:
    rf_model_candidate = train_random_forest_regressor(X_train, y_train, n_estimators=n, random_state=42)
    y_pred_rf_candidate = predict_random_forest_regressor(rf_model_candidate, X_test)
    candidate_metrics = evaluate_regression(y_test, y_pred_rf_candidate)
    if candidate_metrics['mse'] < best_mse:
        best_mse = candidate_metrics['mse']
        best_n_estimators = n

rf_model = train_random_forest_regressor(X_train, y_train, n_estimators=best_n_estimators, random_state=42)
y_pred_rf = predict_random_forest_regressor(rf_model, X_test)
rf_metrics = evaluate_regression(y_test, y_pred_rf)
logger.info(f"Random Forest trained and evaluated with n_estimators={best_n_estimators}.")

# CNN for Regression (assuming sequences are pre-encoded or use a real dataset)
seq_length = 100
X_train_seq = torch.randn(len(X_train), 4, seq_length)  # Placeholder
X_test_seq = torch.randn(len(X_test), 4, seq_length)
train_dataset = TensorDataset(X_train_seq, torch.tensor(y_train, dtype=torch.float32))
test_dataset = TensorDataset(X_test_seq, torch.tensor(y_test, dtype=torch.float32))

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

cnn_model = CNNRegressor(input_channels=4, seq_len=seq_length)
train_deep_model(cnn_model, train_loader, val_loader, epochs=5, lr=0.001, device='cpu')
y_pred_cnn = predict_deep_model(cnn_model, test_loader, device='cpu')
cnn_metrics = evaluate_regression(y_test, np.array(y_pred_cnn))
logger.info("CNN model trained and evaluated.")

# Ensemble predictions
y_pred_ensemble = (y_pred_lr + y_pred_rf + y_pred_cnn) / 3
ensemble_metrics = evaluate_regression(y_test, y_pred_ensemble)
logger.info("Ensemble predictions evaluated.")

# Generate plots
os.makedirs("report/regression_plots", exist_ok=True)
plot_distribution(y, "Distribution of Scores", "report/regression_plots/score_distribution.png")
plot_pred_vs_actual(y_test, y_pred_lr, "Linear Regression Predictions", "report/regression_plots/lr_pred_vs_actual.png")
plot_pred_vs_actual(y_test, y_pred_rf, "Random Forest Predictions", "report/regression_plots/rf_pred_vs_actual.png")
plot_pred_vs_actual(y_test, y_pred_cnn, "CNN Predictions", "report/regression_plots/cnn_pred_vs_actual.png")
plot_pred_vs_actual(y_test, y_pred_ensemble, "Ensemble Predictions", "report/regression_plots/ensemble_pred_vs_actual.png")

# Save metrics and plots as JSON for combined report
regression_results = {
    "Linear Regression": lr_metrics,
    "Random Forest": rf_metrics,
    "CNN": cnn_metrics,
    "Ensemble": ensemble_metrics
}

plots = {
    "score_distribution": "report/regression_plots/score_distribution.png",
    "lr_pred_vs_actual": "report/regression_plots/lr_pred_vs_actual.png",
    "rf_pred_vs_actual": "report/regression_plots/rf_pred_vs_actual.png",
    "cnn_pred_vs_actual": "report/regression_plots/cnn_pred_vs_actual.png",
    "ensemble_pred_vs_actual": "report/regression_plots/ensemble_pred_vs_actual.png"
}

with open("report/regression_metrics.json", 'w') as f:
    json.dump(regression_results, f, indent=4)

with open("report/regression_plots.json", 'w') as f:
    json.dump(plots, f, indent=4)

logger.info("Regression metrics and plots saved as JSON.")
logger.info("Regression pipeline completed.")


================================================================================
FILE: src/pipelines/run_combined.py
================================================================================
# run_combined.py
import json
import os
from src.reporting.generate_report import generate_html_report

# Assume `run_regression.py` and `run_classification.py` have been updated 
# to save their metrics and plot paths to JSON files for easier integration.

# 1. Run regression pipeline
os.system("python -m src.pipelines.run_regression")

# 2. Run classification pipeline
os.system("python -m src.pipelines.run_classification")

# 3. Load saved metrics and plots
with open("report/regression_metrics.json", 'r') as f:
    regression_results = json.load(f)

with open("report/regression_plots.json", 'r') as f:
    regression_plots = json.load(f)

with open("report/classification_metrics.json", 'r') as f:
    classification_results = json.load(f)

with open("report/classification_plots.json", 'r') as f:
    classification_plots = json.load(f)

# Combine plots (if you wish to display all)
all_plots = {**regression_plots, **classification_plots}

# Generate a combined report
generate_html_report(
    regression_results=regression_results,
    classification_results=classification_results,
    plots=all_plots,
    output_path="report/combined_report.html"
)

print("Combined report generated at: report/combined_report.html")


================================================================================
FILE: src/pipelines/run_classification.py
================================================================================
# run_classification.py
import os
import json
import logging
import numpy as np
from sklearn.model_selection import train_test_split
from src.config import FEATURE_DATA_CSV
from src.utils.file_utils import load_csv
from src.models.classical import (
    train_logistic_regression, predict_logistic_regression,
    train_random_forest_classifier, predict_random_forest_classifier
)
from src.evaluation.metrics import evaluate_classification
from src.evaluation.visualization import plot_distribution, plot_roc_curve
import torch
from torch.utils.data import TensorDataset, DataLoader

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting classification pipeline...")

# Load feature data
df = load_csv(FEATURE_DATA_CSV)
logger.info("Features loaded successfully.")

# Binary classification target
y = (df['score'].values > 0.5).astype(int)
X = df.drop(columns=['chrom', 'start', 'end', 'score']).values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
logger.info(f"Data split into train ({len(X_train)}) and test ({len(X_test)}) samples.")

# Logistic Regression
lr_class_model = train_logistic_regression(X_train, y_train)
y_pred_proba_lr = predict_logistic_regression(lr_class_model, X_test)
lr_class_metrics = evaluate_classification(y_test, y_pred_proba_lr)
logger.info("Logistic Regression trained and evaluated.")

# Random Forest Classifier with simple tuning
best_auc = 0.0
best_n_estimators = 100
for n in [100, 200]:
    rf_model_candidate = train_random_forest_classifier(X_train, y_train, n_estimators=n, random_state=42)
    y_pred_proba_rf_cand = predict_random_forest_classifier(rf_model_candidate, X_test)
    candidate_metrics = evaluate_classification(y_test, y_pred_proba_rf_cand)
    if candidate_metrics['auc'] > best_auc:
        best_auc = candidate_metrics['auc']
        best_n_estimators = n

rf_class_model = train_random_forest_classifier(X_train, y_train, n_estimators=best_n_estimators, random_state=42)
y_pred_proba_rf = predict_random_forest_classifier(rf_class_model, X_test)
rf_class_metrics = evaluate_classification(y_test, y_pred_proba_rf)
logger.info(f"Random Forest Classifier trained and evaluated with n_estimators={best_n_estimators}.")

# CNN Classifier placeholder (assume a CNNClassifier model)
# For demonstration, use random predictions
y_pred_proba_cnn = np.random.rand(len(y_test))
cnn_class_metrics = evaluate_classification(y_test, y_pred_proba_cnn)
logger.info("CNN classifier evaluated with placeholder predictions.")

# Ensemble
y_pred_proba_ensemble = (y_pred_proba_lr + y_pred_proba_rf + y_pred_proba_cnn) / 3
ensemble_class_metrics = evaluate_classification(y_test, y_pred_proba_ensemble)
logger.info("Ensemble classification predictions evaluated.")

os.makedirs("report/classification_plots", exist_ok=True)
plot_distribution(y, "Distribution of Class Labels", "report/classification_plots/class_label_distribution.png")
plot_roc_curve(y_test, y_pred_proba_lr, "LR ROC Curve", "report/classification_plots/lr_roc.png")
plot_roc_curve(y_test, y_pred_proba_rf, "RF ROC Curve", "report/classification_plots/rf_roc.png")
plot_roc_curve(y_test, y_pred_proba_cnn, "CNN ROC Curve", "report/classification_plots/cnn_roc.png")
plot_roc_curve(y_test, y_pred_proba_ensemble, "Ensemble ROC Curve", "report/classification_plots/ensemble_roc.png")

classification_results = {
    "Logistic Regression": lr_class_metrics,
    "Random Forest": rf_class_metrics,
    "CNN": cnn_class_metrics,
    "Ensemble": ensemble_class_metrics
}

classification_plots = {
    "class_label_distribution": "report/classification_plots/class_label_distribution.png",
    "lr_roc": "report/classification_plots/lr_roc.png",
    "rf_roc": "report/classification_plots/rf_roc.png",
    "cnn_roc": "report/classification_plots/cnn_roc.png",
    "ensemble_roc": "report/classification_plots/ensemble_roc.png"
}

with open("report/classification_metrics.json", 'w') as f:
    json.dump(classification_results, f, indent=4)

with open("report/classification_plots.json", 'w') as f:
    json.dump(classification_plots, f, indent=4)

logger.info("Classification metrics and plots saved as JSON.")
logger.info("Classification pipeline completed.")

