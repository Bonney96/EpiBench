
================================================================================
FILE: snapshots/txt_snap.py
================================================================================
import os
from datetime import datetime

# Root directory containing the code
ROOT_DIR = "/storage1/fs1/dspencer/Active/spencerlab/abonney/EpiBench"

# File extensions to include in the snapshot
FILE_EXTENSIONS = ['.py', '.sh', '.R', '.ipynb', '.yml', '.yaml']

def should_include(file_name):
    # Exclude macOS resource fork files that start with '._'
    if file_name.startswith("._"):
        return False
    # Include file if it has one of the allowed extensions
    return any(file_name.lower().endswith(ext) for ext in FILE_EXTENSIONS)

def create_text_snapshot(root_dir, extensions):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_name = f"code_snapshot_{timestamp}.txt"
    snapshot_path = os.path.join(os.getcwd(), snapshot_name)

    with open(snapshot_path, 'w', encoding='utf-8', errors='replace') as out_file:
        # Recursively walk the root directory
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if should_include(file):
                    full_path = os.path.join(root, file)
                    # Write a header for each file before its content
                    relative_path = os.path.relpath(full_path, start=root_dir)
                    out_file.write("\n" + "="*80 + "\n")
                    out_file.write(f"FILE: {relative_path}\n")
                    out_file.write("="*80 + "\n")

                    # Append file contents
                    try:
                        with open(full_path, 'r', encoding='utf-8', errors='replace') as f:
                            content = f.read()
                        out_file.write(content)
                        out_file.write("\n")  # Blank line after file content
                    except Exception as e:
                        out_file.write(f"\n[Error reading file: {e}]\n")

    print(f"Text snapshot created: {snapshot_path}")

if __name__ == "__main__":
    create_text_snapshot(ROOT_DIR, FILE_EXTENSIONS)


================================================================================
FILE: snapshots/snapshot.py
================================================================================
import os
import tarfile
from datetime import datetime

# Root directory containing the code
ROOT_DIR = "/storage1/fs1/dspencer/Active/spencerlab/abonney/EpiBench"

# File extensions to include in the snapshot
FILE_EXTENSIONS = ['.py', '.sh', '.R', '.ipynb', '.yml', '.yaml', '.json', '.txt', '.md']

def should_include(file_name):
    # Include file if it has one of the allowed extensions
    return any(file_name.lower().endswith(ext) for ext in FILE_EXTENSIONS)

def create_snapshot(root_dir, extensions):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_name = f"code_snapshot_{timestamp}.tar.gz"
    snapshot_path = os.path.join(os.getcwd(), snapshot_name)

    # Open the tar.gz file for writing
    with tarfile.open(snapshot_path, "w:gz") as tar:
        # Recursively walk the root directory
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if should_include(file):
                    full_path = os.path.join(root, file)
                    # Add file to the archive, preserving directory structure relative to ROOT_DIR
                    arcname = os.path.relpath(full_path, start=root_dir)
                    tar.add(full_path, arcname=arcname)

    print(f"Snapshot created: {snapshot_path}")

if __name__ == "__main__":
    create_snapshot(ROOT_DIR, FILE_EXTENSIONS)


================================================================================
FILE: src/__init__.py
================================================================================
# src/__init__.py
# This top-level __init__ can import frequently used items to make them accessible directly from `src`.
# For example, import the config variables at the top level.

from .config import (
    RAW_BED_FILE,
    RAW_FASTA_FILE,
    PREPARED_DATA_CSV,
    FEATURE_DATA_CSV,
    RANDOM_SEED
)

# This way, I can do: from src import PREPARED_DATA_CSV


================================================================================
FILE: src/config.py
================================================================================
# config.py

##########################
# File Paths
##########################
RAW_BED_FILE = "data/raw/ND150826-CD34-wgbs.positive_control_regions.meth.bed"
RAW_FASTA_FILE = "data/raw/positive_control_sequences.fa"
PREPARED_DATA_CSV = "data/processed/positive_control_data_prepared.csv"
FEATURE_DATA_CSV = "data/processed/positive_control_features.csv"

##########################
# General Settings
##########################
RANDOM_SEED = 42  # For reproducibility


================================================================================
FILE: src/reporting/__init__.py
================================================================================
# src/reporting/__init__.py
from .generate_report import generate_html_report


================================================================================
FILE: src/reporting/generate_report.py
================================================================================
# generate_report.py
import os

def generate_html_report(
    regression_results,
    classification_results,
    data_exploration_plots,
    plots,
    output_path
):
    """
    Generate a comprehensive HTML report comparing multiple regression and
    classification models, as well as providing data and feature exploration.

    Assumptions:
    - `output_path` is something like "report/index.html".
    - All paths in `data_exploration_plots` and `plots` are relative to the directory of `output_path`.
    """

    # Construct the regression results table
    regression_table = (
        "<table class='metric-table'>"
        "<tr><th>Model</th><th>MSE</th><th>RÂ²</th><th>Pearson Corr</th></tr>"
    )
    for model, metrics in regression_results.items():
        regression_table += (
            f"<tr><td>{model}</td>"
            f"<td>{metrics['mse']:.4f}</td>"
            f"<td>{metrics['r2']:.4f}</td>"
            f"<td>{metrics['pearson_corr']:.4f}</td></tr>"
        )
    regression_table += "</table>"

    # Construct the classification results table
    classification_table = (
        "<table class='metric-table'>"
        "<tr><th>Model</th><th>AUC</th><th>Accuracy</th>"
        "<th>F1</th><th>Precision</th><th>Recall</th></tr>"
    )
    for model, metrics in classification_results.items():
        classification_table += (
            f"<tr><td>{model}</td>"
            f"<td>{metrics['auc']:.4f}</td>"
            f"<td>{metrics['accuracy']:.4f}</td>"
            f"<td>{metrics['f1']:.4f}</td>"
            f"<td>{metrics['precision']:.4f}</td>"
            f"<td>{metrics['recall']:.4f}</td></tr>"
        )
    classification_table += "</table>"

    # Construct the data exploration section
    data_exploration_html = "<h2>Data and Feature Exploration</h2>"

    # Add correlation heatmap if available
    if "correlation_heatmap" in data_exploration_plots:
        corr_path = data_exploration_plots["correlation_heatmap"]
        data_exploration_html += (
            "<h3>Feature Correlation Heatmap</h3>"
            f"<img src='{corr_path}' alt='Correlation Heatmap'>"
        )

    # Add feature distributions if available
    if "feature_distributions" in data_exploration_plots:
        data_exploration_html += "<h3>Feature Distributions</h3>"
        for feature, fpath in data_exploration_plots["feature_distributions"].items():
            data_exploration_html += (
                f"<h4>{feature}</h4>"
                f"<img src='{fpath}' alt='{feature}_distribution'>"
            )

    # Construct the model performance plots section
    images_html = "<h2>Model Performance Plots</h2>"
    for plot_name, plot_path in plots.items():
        formatted_name = plot_name.replace('_', ' ').title()
        images_html += f"<h3>{formatted_name}</h3><img src='{plot_path}' alt='{plot_name}'>"

    # Final HTML content
    html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Methylation Prediction Report</title>
<style>
body {{
    font-family: Arial, sans-serif;
    margin: 20px;
}}
h1 {{
    color: #333;
}}
.metric-table {{
    border-collapse: collapse;
    margin: 20px 0;
    width: 60%;
}}
.metric-table th, .metric-table td {{
    border: 1px solid #ccc;
    padding: 8px 12px;
    text-align: center;
}}
.metric-table th {{
    background-color: #f4f4f4;
}}
img {{
    max-width: 600px;
    height: auto;
    display: block;
    margin: 20px 0;
}}
</style>
</head>
<body>
<h1>Methylation Prediction Report</h1>
<p>This report compares multiple models for both regression and classification tasks, and also provides an overview of the underlying data and extracted features.</p>

{data_exploration_html}

<h2>Regression Performance</h2>
{regression_table}

<h2>Classification Performance</h2>
{classification_table}

{images_html}

<p>The Random Forest approach often provides strong performance, as seen in both regression and classification results. The data exploration section illustrates the relationships between features and their distributions.</p>

</body>
</html>
"""

    # Ensure output directory exists
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # Write the HTML file
    with open(output_path, 'w') as f:
        f.write(html_content)

    print(f"Report generated at: {output_path}")


================================================================================
FILE: src/utils/__init__.py
================================================================================
# src/utils/__init__.py
# Re-export utilities for easy access.
from .file_utils import load_csv, save_csv
from .seq_utils import gc_content, cpg_density, kmer_counts


================================================================================
FILE: src/utils/seq_utils.py
================================================================================
# seq_utils.py
from itertools import product

def gc_content(seq):
    seq = seq.upper()
    gc_count = seq.count('G') + seq.count('C')
    return gc_count / len(seq) if len(seq) > 0 else 0

def cpg_density(seq):
    seq = seq.upper()
    cpg_count = seq.count("CG")
    return cpg_count / len(seq) if len(seq) > 0 else 0

def kmer_counts(seq, k=2):
    seq = seq.upper()
    counts = {}
    possible_kmers = [''.join(p) for p in product('ACGT', repeat=k)]
    for km in possible_kmers:
        counts[km] = 0
    
    for i in range(len(seq)-k+1):
        kmer = seq[i:i+k]
        if kmer in counts:
            counts[kmer] += 1
    
    length = len(seq)
    if length > 0:
        for km in counts:
            counts[km] = counts[km] / (length - k + 1)
    return counts


================================================================================
FILE: src/utils/file_utils.py
================================================================================
# file_utils.py
import pandas as pd
import os

def load_csv(path):
    """
    Load a CSV file into a pandas DataFrame.
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"File not found: {path}")
    return pd.read_csv(path)

def save_csv(df, path, index=False):
    """
    Save a pandas DataFrame to a CSV file.
    """
    directory = os.path.dirname(path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory)
    df.to_csv(path, index=index)
    print(f"Saved CSV to: {path}")


================================================================================
FILE: src/models/classical.py
================================================================================
# classical.py

import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

def train_linear_regression(X, y):
    model = LinearRegression()
    model.fit(X, y)
    return model

def predict_linear_regression(model, X):
    return model.predict(X)

def train_random_forest_regressor(X, y, n_estimators=100, random_state=42):
    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)
    model.fit(X, y)
    return model

def predict_random_forest_regressor(model, X):
    return model.predict(X)

def train_logistic_regression(X, y):
    model = LogisticRegression(solver='liblinear', random_state=42)
    model.fit(X, y)
    return model

def predict_logistic_regression(model, X):
    # return probabilities for classification tasks
    return model.predict_proba(X)[:, 1]

def train_random_forest_classifier(X, y, n_estimators=100, random_state=42):
    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)
    model.fit(X, y)
    return model

def predict_random_forest_classifier(model, X):
    return model.predict_proba(X)[:, 1]


================================================================================
FILE: src/models/__init__.py
================================================================================
# src/models/__init__.py
# Re-export model training functions and classes to access them easily.

from .classical import (
    train_linear_regression, predict_linear_regression,
    train_random_forest_regressor, predict_random_forest_regressor,
    train_logistic_regression, predict_logistic_regression,
    train_random_forest_classifier, predict_random_forest_classifier
)

from .deep_learning import (
    MLPRegressor, train_model, predict_model,
)

from .datasets import SequenceDataset


================================================================================
FILE: src/models/deep_learning.py
================================================================================
# optimized_regression.py

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import numpy as np

# Example Dataset class
class MethylationDataset(Dataset):
    def __init__(self, X, y):
        self.X = X.astype(np.float32)
        self.y = y.astype(np.float32)
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Fully Connected Regressor
class MLPRegressor(nn.Module):
    def __init__(self, input_dim, hidden_dims=[128, 64], dropout=0.3):
        super(MLPRegressor, self).__init__()
        layers = []
        prev_dim = input_dim
        for h in hidden_dims:
            layers.append(nn.Linear(prev_dim, h))
            layers.append(nn.BatchNorm1d(h))
            layers.append(nn.ReLU())
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            prev_dim = h
        layers.append(nn.Linear(prev_dim, 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

def train_model(model, train_loader, val_loader, epochs=50, lr=0.001, weight_decay=1e-5, device='cpu', early_stop_patience=5):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    model.to(device)

    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device).unsqueeze(1)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device).unsqueeze(1)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

        # Early stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), "best_model.pth")
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

def predict_model(model, test_loader, device='cpu'):
    model.to(device)
    model.eval()
    preds = []
    with torch.no_grad():
        for X_batch, _ in test_loader:
            X_batch = X_batch.to(device)
            outputs = model(X_batch)
            preds.extend(outputs.squeeze(1).cpu().numpy())
    return np.array(preds)

# ============================
# Example Training Script
# ============================
if __name__ == "__main__":
    # Suppose you have loaded your feature matrix X and target y
    # X: shape (N, num_features)
    # y: shape (N,)
    # For demonstration, we create random data:
    N = 2686
    num_features = 85  # e.g., from the provided k2, k3 features, etc.
    X = np.random.rand(N, num_features).astype(np.float32)
    y = np.random.rand(N).astype(np.float32)

    # (1) Feature Scaling
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # (2) Optional PCA to reduce dimensionality and handle correlations
    # Adjust n_components as needed.
    # If correlations are strong, PCA might help.
    pca = PCA(n_components=50)  # for example, reduce to 50 features
    X_pca = pca.fit_transform(X_scaled)

    # Create Dataset
    dataset = MethylationDataset(X_pca, y)

    # Split into train/val/test (e.g., 70/15/15)
    train_size = int(0.7 * N)
    val_size = int(0.15 * N)
    test_size = N - train_size - val_size

    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

    # Dataloaders
    batch_size = 32
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # Initialize and train model
    model = MLPRegressor(input_dim=50, hidden_dims=[128, 64], dropout=0.3)
    train_model(model, train_loader, val_loader, epochs=50, lr=0.001, weight_decay=1e-5, device=device, early_stop_patience=5)

    # Load best model
    model.load_state_dict(torch.load("best_model.pth"))

    # Predict on test set
    preds = predict_model(model, test_loader, device=device)
    # Evaluate preds against test targets...
    # e.g., compute MSE or R^2:
    test_targets = np.array([y for _, y in test_dataset])
    mse = ((preds - test_targets)**2).mean()
    print(f"Test MSE: {mse:.4f}")


================================================================================
FILE: src/models/datasets.py
================================================================================
# datasets.py
import torch
from torch.utils.data import Dataset
import numpy as np

class SequenceDataset(Dataset):
    """
    A PyTorch Dataset for sequences.
    """
    def __init__(self, df, seq_length, target_col='score', classification=False, threshold=0.5):
        """
        df: DataFrame containing 'sequence' and 'score' columns, plus features if needed.
        seq_length: Fixed length of sequences.
        classification: If True, prepare binary classification targets.
        threshold: Threshold for binary classification.
        """
        self.df = df.reset_index(drop=True)
        self.seq_length = seq_length
        self.classification = classification
        self.threshold = threshold

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        seq = row['sequence']
        X_seq = self.one_hot_encode(seq, self.seq_length)
        y = row['score']
        if self.classification:
            y = 1 if y > self.threshold else 0
        return X_seq, y

    def one_hot_encode(self, seq, fixed_len):
        seq = seq.upper()
        mapping = {'A':0, 'C':1, 'G':2, 'T':3}
        arr = np.zeros((4, fixed_len), dtype=np.float32)
        seq = seq[:fixed_len]
        if len(seq) < fixed_len:
            seq = seq + 'A'*(fixed_len - len(seq))
        for i, base in enumerate(seq):
            arr[mapping.get(base, 0), i] = 1.0
        return arr


================================================================================
FILE: src/data_preparation/prepare_data.py
================================================================================
import sys
from Bio import SeqIO

# Update these paths as needed
bed_file = "/storage1/fs1/dspencer/Active/spencerlab/abonney/regions/simple_project/ND150826-CD34-wgbs.positive_control_regions.meth.bed"
fasta_file = "/storage1/fs1/dspencer/Active/spencerlab/abonney/regions/simple_project/positive_control_sequences.fa"
output_csv = "positive_control_data.csv"

# Read BED data
# Based on the data you showed, columns are:
# 0: chrom
# 1: start
# 2: end
# 3: coverage (not needed)
# 4: score (between 0 and 1)
regions = []
with open(bed_file, 'r') as bf:
    for line in bf:
        if line.strip():
            parts = line.strip().split('\t')
            chrom = parts[0]
            start = parts[1]
            end = parts[2]
            # coverage = parts[3]  # If you need it, but not required now
            score = float(parts[4])
            regions.append((chrom, start, end, score))

# Create a dictionary keyed by region string (chr:start-end) to score
score_dict = {}
for (chrom, start, end, score) in regions:
    region_key = f"{chrom}:{start}-{end}"
    score_dict[region_key] = score

# Parse FASTA and match sequences to scores
with open(output_csv, 'w') as out:
    out.write("chrom,start,end,score,sequence\n")
    for record in SeqIO.parse(fasta_file, "fasta"):
        # record.id should be something like: chr1:10000-10100
        region = record.id
        seq = str(record.seq)
        chrom = region.split(':')[0]
        coords = region.split(':')[1]
        start = coords.split('-')[0]
        end = coords.split('-')[1]
        score = score_dict.get(region, "unknown")
        out.write(f"{chrom},{start},{end},{score},{seq}\n")


================================================================================
FILE: src/data_preparation/__init__.py
================================================================================
# src/data_preparation/__init__.py

from .prepare_data import main as prepare_data_main
from .feature_engineering import main as feature_engineering_main

================================================================================
FILE: src/data_preparation/feature_engineering.py
================================================================================
# feature_engineering.py
import pandas as pd
from src.config import PREPARED_DATA_CSV, FEATURE_DATA_CSV
from src.utils.seq_utils import gc_content, cpg_density, kmer_counts
from itertools import product

df = pd.read_csv(PREPARED_DATA_CSV)

all_features = []
for _, row in df.iterrows():
    seq = row['sequence']
    features = {
        'chrom': row['chrom'],
        'start': row['start'],
        'end': row['end'],
        'score': row['score'],
        'seq_length': len(seq),
        'gc_content': gc_content(seq),
        'cpg_density': cpg_density(seq)
    }
    
    # Add 2-mer frequencies
    k2_counts = kmer_counts(seq, k=2)
    for kmer, val in k2_counts.items():
        features[f"k2_{kmer}"] = val
    
    # Add 3-mer frequencies
    k3_counts = kmer_counts(seq, k=3)
    for kmer, val in k3_counts.items():
        features[f"k3_{kmer}"] = val
    
    all_features.append(features)

feature_df = pd.DataFrame(all_features)
feature_df.to_csv(FEATURE_DATA_CSV, index=False)


================================================================================
FILE: src/evaluation/__init__.py
================================================================================
# src/evaluation/__init__.py
from .metrics import evaluate_regression, evaluate_classification
from .visualization import (
    plot_distribution, plot_pred_vs_actual, plot_roc_curve, plot_feature_distributions, plot_correlation_heatmap
)


================================================================================
FILE: src/evaluation/visualization.py
================================================================================
# visualization.py

import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score


def plot_distribution(y, title, save_path):
    plt.figure(figsize=(6,4))
    sns.histplot(y, kde=True, color='blue', bins=20)
    plt.title(title)
    plt.xlabel("Value")
    plt.ylabel("Count")
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def plot_pred_vs_actual(y_true, y_pred, title, save_path):
    plt.figure(figsize=(6,6))
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([0,1],[0,1], 'r--')
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.title(title)
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def plot_roc_curve(y_true, y_pred_proba, title, save_path):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6,6))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC area = {roc_auc:.2f}')
    plt.plot([0,1],[0,1], 'r--')
    plt.xlim([0,1])
    plt.ylim([0,1])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(title)
    plt.legend(loc="lower right")
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def _ensure_dir_exists(path):
    dir_name = os.path.dirname(path)
    if dir_name and not os.path.exists(dir_name):
        os.makedirs(dir_name)

def plot_correlation_heatmap(df, title, save_path):
    plt.figure(figsize=(10,8))
    corr = df.corr()
    sns.heatmap(corr, annot=False, cmap='coolwarm', square=True)
    plt.title(title)
    plt.tight_layout()
    _ensure_dir_exists(save_path)
    plt.savefig(save_path, dpi=150)
    plt.close()

def plot_feature_distributions(df, features, output_dir):
    # Create a directory for the individual feature plots
    os.makedirs(output_dir, exist_ok=True)
    plot_paths = {}
    for feature in features:
        plt.figure(figsize=(6,4))
        sns.histplot(df[feature], kde=True, color='blue', bins=20)
        plt.title(f"Distribution of {feature}")
        plt.xlabel(feature)
        plt.ylabel("Count")
        plt.tight_layout()
        save_path = os.path.join(output_dir, f"{feature}_distribution.png")
        plt.savefig(save_path, dpi=150)
        plt.close()
        plot_paths[feature] = save_path
    return plot_paths

================================================================================
FILE: src/evaluation/metrics.py
================================================================================
# metrics.py

from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score
from scipy.stats import pearsonr
import numpy as np

def evaluate_regression(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    if np.std(y_pred) == 0 or np.std(y_true) == 0:
        corr = 0.0
    else:
        corr, _ = pearsonr(y_true, y_pred)
    return {
        'mse': mse,
        'r2': r2,
        'pearson_corr': corr
    }

def evaluate_classification(y_true, y_pred_proba, threshold=0.5):
    y_pred = (y_pred_proba >= threshold).astype(int)
    auc = roc_auc_score(y_true, y_pred_proba)
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    return {
        'auc': auc,
        'accuracy': acc,
        'f1': f1,
        'precision': prec,
        'recall': rec
    }


================================================================================
FILE: src/pipelines/run_data_exploration.py
================================================================================
# run_data_exploration.py

import os
import json
import logging
import pandas as pd
from src.config import FEATURE_DATA_CSV
from src.utils.file_utils import load_csv
from src.evaluation.visualization import plot_correlation_heatmap, plot_feature_distributions

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting data and feature exploration pipeline...")

# Load the feature data
df = load_csv(FEATURE_DATA_CSV)
logger.info("Feature data loaded successfully.")

exclude_cols = ['chrom', 'start', 'end', 'score']
if 'sequence' in df.columns:
    exclude_cols.append('sequence')

numeric_features = [col for col in df.columns if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]

# Create output directories
os.makedirs("report/data_exploration_plots", exist_ok=True)

# Plot correlation heatmap of numeric features
corr_heatmap_path_full = "report/data_exploration_plots/feature_correlation_heatmap.png"
plot_correlation_heatmap(df[numeric_features], "Feature Correlation Heatmap", corr_heatmap_path_full)

# Plot distributions of each numeric feature
feature_dist_dir_full = "report/data_exploration_plots/features"
feature_dist_plots_full = plot_feature_distributions(df, numeric_features, feature_dist_dir_full)

# Remove the 'report/' prefix for JSON storage
corr_heatmap_path = "data_exploration_plots/feature_correlation_heatmap.png"
feature_dist_plots = {f: p.replace("report/", "") for f, p in feature_dist_plots_full.items()}

data_exploration_plots = {
    "correlation_heatmap": corr_heatmap_path,
    "feature_distributions": feature_dist_plots
}

# Save to JSON
with open("report/data_exploration_plots.json", 'w') as f:
    json.dump(data_exploration_plots, f, indent=4)

logger.info("Data exploration plots saved as JSON.")

# Generate text-based summary report
summary_report_path = "report/data_exploration_summary.txt"

desc = df[numeric_features].describe(include='all')
corr_matrix = df[numeric_features].corr()

corr_pairs = corr_matrix.unstack().dropna()
corr_pairs = corr_pairs[corr_pairs.index.get_level_values(0) != corr_pairs.index.get_level_values(1)]
top_positive_correlations = corr_pairs.sort_values(ascending=False).head(10)
top_negative_correlations = corr_pairs.sort_values(ascending=True).head(10)

with open(summary_report_path, "w") as report_file:
    report_file.write("=== Data Exploration Summary ===\n\n")
    report_file.write("**Numeric Features:**\n")
    report_file.write(", ".join(numeric_features) + "\n\n")

    report_file.write("**Summary Statistics (Numeric Features):**\n")
    report_file.write(desc.to_string() + "\n\n")

    report_file.write("**Top 10 Positive Correlations:**\n")
    for (f1, f2), value in top_positive_correlations.items():
        report_file.write(f"{f1} and {f2}: {value:.3f}\n")
    report_file.write("\n")

    report_file.write("**Top 10 Negative Correlations:**\n")
    for (f1, f2), value in top_negative_correlations.items():
        report_file.write(f"{f1} and {f2}: {value:.3f}\n")
    report_file.write("\n")

    report_file.write("**Observations: **\n")
    report_file.write("1. Review correlations to identify strong relationships.\n")
    report_file.write("2. The summary statistics provide distribution insights.\n")
    report_file.write("3. Feature distribution plots are in 'report/data_exploration_plots/features'.\n")

logger.info(f"Data exploration summary saved at {summary_report_path}.")
logger.info("Data exploration pipeline completed.")


================================================================================
FILE: src/pipelines/run_regression.py
================================================================================
# run_regression.py

import os
import json
import logging
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import TensorDataset, DataLoader
from src.config import FEATURE_DATA_CSV
from src.utils.file_utils import load_csv
from src.models.classical import (
    train_linear_regression, predict_linear_regression,
    train_random_forest_regressor, predict_random_forest_regressor
)
from src.models.deep_learning import MLPRegressor, train_model, predict_model
from src.evaluation.metrics import evaluate_regression
from src.evaluation.visualization import plot_distribution, plot_pred_vs_actual
import torch

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting regression pipeline...")

df = load_csv(FEATURE_DATA_CSV)
logger.info("Features loaded successfully.")

y = df['score'].values
X = df.drop(columns=['chrom', 'start', 'end', 'score']).values

X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
logger.info(f"Data split into train ({len(X_train_full)}) and test ({len(X_test)}) samples.")

scaler = StandardScaler()
X_train_full = scaler.fit_transform(X_train_full)
X_test = scaler.transform(X_test)

val_ratio = 0.2
val_size = int(len(X_train_full) * val_ratio)
train_size = len(X_train_full) - val_size

X_train = X_train_full[:train_size]
y_train = y_train_full[:train_size]
X_val = X_train_full[train_size:]
y_val = y_train_full[train_size:]

# Linear Regression
lr_model = train_linear_regression(X_train, y_train)
y_pred_lr = predict_linear_regression(lr_model, X_test)
lr_metrics = evaluate_regression(y_test, y_pred_lr)
logger.info("Linear Regression trained and evaluated.")

# Random Forest
best_mse = float('inf')
best_n_estimators = 100
for n in [100, 200, 300]:
    rf_model_candidate = train_random_forest_regressor(X_train, y_train, n_estimators=n, random_state=42)
    y_pred_rf_candidate = predict_random_forest_regressor(rf_model_candidate, X_test)
    candidate_metrics = evaluate_regression(y_test, y_pred_rf_candidate)
    if candidate_metrics['mse'] < best_mse:
        best_mse = candidate_metrics['mse']
        best_n_estimators = n

rf_model = train_random_forest_regressor(X_train, y_train, n_estimators=best_n_estimators, random_state=42)
y_pred_rf = predict_random_forest_regressor(rf_model, X_test)
rf_metrics = evaluate_regression(y_test, y_pred_rf)
logger.info(f"Random Forest trained and evaluated with n_estimators={best_n_estimators}.")

# MLP
device = 'cuda' if torch.cuda.is_available() else 'cpu'
X_train_t = torch.tensor(X_train, dtype=torch.float32)
y_train_t = torch.tensor(y_train, dtype=torch.float32)
X_val_t = torch.tensor(X_val, dtype=torch.float32)
y_val_t = torch.tensor(y_val, dtype=torch.float32)
X_test_t = torch.tensor(X_test, dtype=torch.float32)
y_test_t = torch.tensor(y_test, dtype=torch.float32)

train_dataset = TensorDataset(X_train_t, y_train_t)
val_dataset = TensorDataset(X_val_t, y_val_t)
test_dataset = TensorDataset(X_test_t, y_test_t)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

input_dim = X_train.shape[1]
mlp_model = MLPRegressor(input_dim=input_dim, hidden_dims=[128, 64], dropout=0.3)
train_model(mlp_model, train_loader, val_loader, epochs=50, lr=0.001, device=device, early_stop_patience=5)

mlp_model.load_state_dict(torch.load("best_model.pth"))
y_pred_mlp = predict_model(mlp_model, test_loader, device=device)
mlp_metrics = evaluate_regression(y_test, y_pred_mlp)
logger.info("MLP model trained and evaluated.")

# Ensemble
y_pred_ensemble = (y_pred_lr + y_pred_rf + y_pred_mlp) / 3
ensemble_metrics = evaluate_regression(y_test, y_pred_ensemble)
logger.info("Ensemble predictions evaluated.")

os.makedirs("report/regression_plots", exist_ok=True)

plot_distribution(y, "Distribution of Scores", "report/regression_plots/score_distribution.png")
plot_pred_vs_actual(y_test, y_pred_lr, "Linear Regression Predictions", "report/regression_plots/lr_pred_vs_actual.png")
plot_pred_vs_actual(y_test, y_pred_rf, "Random Forest Predictions", "report/regression_plots/rf_pred_vs_actual.png")
plot_pred_vs_actual(y_test, y_pred_mlp, "MLP Predictions", "report/regression_plots/mlp_pred_vs_actual.png")
plot_pred_vs_actual(y_test, y_pred_ensemble, "Ensemble Predictions", "report/regression_plots/ensemble_pred_vs_actual.png")

# Store metrics
regression_results = {
    "Linear Regression": lr_metrics,
    "Random Forest": rf_metrics,
    "MLP": mlp_metrics,
    "Ensemble": ensemble_metrics
}

# Remove "report/" prefix for the plots dictionary
plots = {
    "score_distribution": "regression_plots/score_distribution.png",
    "lr_pred_vs_actual": "regression_plots/lr_pred_vs_actual.png",
    "rf_pred_vs_actual": "regression_plots/rf_pred_vs_actual.png",
    "mlp_pred_vs_actual": "regression_plots/mlp_pred_vs_actual.png",
    "ensemble_pred_vs_actual": "regression_plots/ensemble_pred_vs_actual.png"
}

with open("report/regression_metrics.json", 'w') as f:
    json.dump(regression_results, f, indent=4)

with open("report/regression_plots.json", 'w') as f:
    json.dump(plots, f, indent=4)

# Temporary empty classification results for demonstration
classification_results = {}

from src.reporting.generate_report import generate_html_report

# Load data exploration plots (if they exist)
data_exploration_plots = {}
if os.path.exists("report/data_exploration_plots.json"):
    with open("report/data_exploration_plots.json", 'r') as f:
        data_exploration_plots = json.load(f)

generate_html_report(
    regression_results,
    classification_results,
    data_exploration_plots,
    plots,
    output_path="report/index.html"
)

logger.info("Regression pipeline completed, report generated.")


================================================================================
FILE: src/pipelines/run_combined.py
================================================================================
# run_combined.py
import json
import os
from src.reporting.generate_report import generate_html_report

# 1. Run regression pipeline
os.system("python -m src.pipelines.run_regression")

# 2. Run classification pipeline
os.system("python -m src.pipelines.run_classification")

# 3. Run data exploration pipeline
os.system("python -m src.pipelines.run_data_exploration")

# Load saved metrics and plots
with open("report/regression_metrics.json", 'r') as f:
    regression_results = json.load(f)

with open("report/regression_plots.json", 'r') as f:
    regression_plots = json.load(f)

with open("report/classification_metrics.json", 'r') as f:
    classification_results = json.load(f)

with open("report/classification_plots.json", 'r') as f:
    classification_plots = json.load(f)

with open("report/data_exploration_plots.json", 'r') as f:
    data_exploration_plots = json.load(f)

# Combine regression and classification plots and also keep data exploration separate
# Here we do not prepend 'report/' to keep paths relative from index.html
all_plots = {}
all_plots.update(regression_plots)
all_plots.update(classification_plots)

generate_html_report(
    regression_results=regression_results,
    classification_results=classification_results,
    data_exploration_plots=data_exploration_plots,
    plots=all_plots,
    output_path="report/combined_report.html"
)

print("Combined report generated at: report/combined_report.html")


================================================================================
FILE: src/pipelines/run_classification.py
================================================================================
# run_classification.py
import os
import json
import logging
import numpy as np
from sklearn.model_selection import train_test_split
from src.config import FEATURE_DATA_CSV
from src.utils.file_utils import load_csv
from src.models.classical import (
    train_logistic_regression, predict_logistic_regression,
    train_random_forest_classifier, predict_random_forest_classifier
)
from src.evaluation.metrics import evaluate_classification
from src.evaluation.visualization import plot_distribution, plot_roc_curve

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting classification pipeline...")

df = load_csv(FEATURE_DATA_CSV)
logger.info("Features loaded successfully.")

y = (df['score'].values > 0.5).astype(int)
X = df.drop(columns=['chrom', 'start', 'end', 'score']).values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
logger.info(f"Data split into train ({len(X_train)}) and test ({len(X_test)}) samples.")

lr_class_model = train_logistic_regression(X_train, y_train)
y_pred_proba_lr = predict_logistic_regression(lr_class_model, X_test)
lr_class_metrics = evaluate_classification(y_test, y_pred_proba_lr)
logger.info("Logistic Regression trained and evaluated.")

best_auc = 0.0
best_n_estimators = 100
for n in [100, 200]:
    rf_model_candidate = train_random_forest_classifier(X_train, y_train, n_estimators=n, random_state=42)
    y_pred_proba_rf_cand = predict_random_forest_classifier(rf_model_candidate, X_test)
    candidate_metrics = evaluate_classification(y_test, y_pred_proba_rf_cand)
    if candidate_metrics['auc'] > best_auc:
        best_auc = candidate_metrics['auc']
        best_n_estimators = n

rf_class_model = train_random_forest_classifier(X_train, y_train, n_estimators=best_n_estimators, random_state=42)
y_pred_proba_rf = predict_random_forest_classifier(rf_class_model, X_test)
rf_class_metrics = evaluate_classification(y_test, y_pred_proba_rf)
logger.info(f"Random Forest Classifier trained and evaluated with n_estimators={best_n_estimators}.")

# Placeholder MLP model predictions
y_pred_proba_MLP = np.random.rand(len(y_test))
MLP_class_metrics = evaluate_classification(y_test, y_pred_proba_MLP)
logger.info("MLP classifier evaluated with placeholder predictions.")

y_pred_proba_ensemble = (y_pred_proba_lr + y_pred_proba_rf + y_pred_proba_MLP) / 3
ensemble_class_metrics = evaluate_classification(y_test, y_pred_proba_ensemble)
logger.info("Ensemble classification predictions evaluated.")

os.makedirs("report/classification_plots", exist_ok=True)
plot_distribution(y, "Distribution of Class Labels", "report/classification_plots/class_label_distribution.png")
plot_roc_curve(y_test, y_pred_proba_lr, "LR ROC Curve", "report/classification_plots/lr_roc.png")
plot_roc_curve(y_test, y_pred_proba_rf, "RF ROC Curve", "report/classification_plots/rf_roc.png")
plot_roc_curve(y_test, y_pred_proba_MLP, "MLP ROC Curve", "report/classification_plots/mlp_roc.png")
plot_roc_curve(y_test, y_pred_proba_ensemble, "Ensemble ROC Curve", "report/classification_plots/ensemble_roc.png")

classification_results = {
    "Logistic Regression": lr_class_metrics,
    "Random Forest": rf_class_metrics,
    "MLP": MLP_class_metrics,
    "Ensemble": ensemble_class_metrics
}

classification_plots = {
    "class_label_distribution": "classification_plots/class_label_distribution.png",
    "lr_roc": "classification_plots/lr_roc.png",
    "rf_roc": "classification_plots/rf_roc.png",
    "mlp_roc": "classification_plots/mlp_roc.png",
    "ensemble_roc": "classification_plots/ensemble_roc.png"
}

with open("report/classification_metrics.json", 'w') as f:
    json.dump(classification_results, f, indent=4)

with open("report/classification_plots.json", 'w') as f:
    json.dump(classification_plots, f, indent=4)

logger.info("Classification metrics and plots saved as JSON.")
logger.info("Classification pipeline completed.")

